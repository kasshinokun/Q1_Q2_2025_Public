# based on file stCRUDDataObjectPY_v3alpha.py

import streamlit as st
import csv
import os
import struct
from pathlib import Path
import json
from typing import List, Dict, Optional, Union, Callable, Any, Iterator
import tempfile
import logging
from datetime import datetime, date
import traceback
import hashlib
import time  # For simulating delays/retries if needed, and for timing operations
import filelock # For cross-platform file locking

# Configure logging
logging.basicConfig(
    level=logging.INFO, # Set to logging.DEBUG to see detailed date parsing attempts
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('traffic_accidents.log'), # Log to a file
        logging.StreamHandler() # Also log to console
    ]
)
logger = logging.getLogger(__name__)

# --- Constants ---
DB_DIR = os.path.join(Path.home(), 'Documents', 'Data')
DB_FILE = os.path.join(DB_DIR, 'traffic_accidents.db')
INDEX_FILE = os.path.join(DB_DIR, 'index.idx') # New index file
BACKUP_DIR = os.path.join(DB_DIR, 'backups')
LOCK_FILE = os.path.join(DB_DIR, 'traffic_accidents.lock') # Dedicated lock file
CSV_DELIMITER = ';'
MAX_RECORDS_PER_PAGE = 20
MAX_FILE_SIZE_MB = 100 # Maximum CSV file size for import
CHUNK_SIZE = 4096     # Read/write chunk size for file operations (4KB)
MAX_BACKUPS = 5       # Keep only the last N backups
MAX_LOG_ENTRIES_DISPLAY = 10 # Number of log entries to display in activity log

# --- Data Structure ---
# Define the structure of a record:
# - record_id: unique identifier (UUID-like, generated by hashing data)
# - timestamp: datetime of creation/last update
# - data_hash: SHA256 hash of the data content
# - is_valid: boolean, True if active, False if logically deleted
# - data: JSON string of the actual accident data
RECORD_FORMAT = "<32s d 32s ?" # record_id (32 bytes for hex hash), timestamp (double), data_hash (32 bytes for hex hash), is_valid (boolean)
RECORD_HEADER_SIZE = struct.calcsize(RECORD_FORMAT)
# A full record in the .db file will be:
# RECORD_HEADER (fixed size) + data_size (int) + data (bytes)

# --- Database Class ---
class TrafficAccidentsDB:
    def __init__(self, db_file: str, index_file: str, lock_file: str):
        self.db_file = db_file
        self.index_file = index_file
        self.lock_file = lock_file
        self._ensure_db_and_index_dirs()
        self._initialize_db_file()
        self.rebuild_index() # Rebuild index on initialization to ensure consistency

    def _ensure_db_and_index_dirs(self):
        """Ensures the database and index directories exist."""
        try:
            os.makedirs(os.path.dirname(self.db_file), exist_ok=True)
            os.makedirs(os.path.dirname(self.index_file), exist_ok=True) # Ensure index dir exists
        except OSError as e:
            logger.critical(f"Critical: Cannot create database directories. Error: {e}")
            raise

    def _initialize_db_file(self):
        """Creates the database file if it doesn't exist."""
        if not os.path.exists(self.db_file):
            try:
                with open(self.db_file, 'wb') as f:
                    pass  # Create an empty file
                logger.info(f"Database file created: {self.db_file}")
            except IOError as e:
                logger.critical(f"Critical: Cannot create database file {self.db_file}. Error: {e}")
                raise

    def _generate_record_id(self, data: Dict[str, Any]) -> str:
        """Generates a unique ID for a record based on its content."""
        # Use a combination of timestamp and data hash to make it more unique
        # For simplicity, let's just hash the JSON string of the data for now.
        # A more robust solution might involve UUIDs or a dedicated ID generation service.
        data_string = json.dumps(data, sort_keys=True)
        return hashlib.sha256(data_string.encode('utf-8')).hexdigest()

    def _calculate_data_hash(self, data: Dict[str, Any]) -> str:
        """Calculates SHA256 hash of the data content."""
        data_string = json.dumps(data, sort_keys=True)
        return hashlib.sha256(data_string.encode('utf-8')).hexdigest()

    def _write_index_entry(self, record_id: str, position: int):
        """
        Writes or updates an entry in the index file.
        This function now works by:
        1. Reading the entire index into memory.
        2. Updating the specific record_id's position.
        3. Rewriting the entire index file.
        This is suitable for smaller indexes. For very large indexes, a more
        sophisticated approach (e.g., in-place update if size is fixed, or a B-tree)
        would be required.
        """
        # The index file will store record_id (32 bytes) and position (long long - 8 bytes)
        # Format: <32s Q
        # Q is for unsigned long long, suitable for file offsets
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                index_data = {}
                if os.path.exists(self.index_file):
                    try:
                        with open(self.index_file, 'rb') as f_idx:
                            entry_size = struct.calcsize("<32s Q")
                            while True:
                                chunk = f_idx.read(entry_size)
                                if not chunk:
                                    break
                                if len(chunk) == entry_size:
                                    r_id_bytes, pos = struct.unpack("<32s Q", chunk)
                                    r_id = r_id_bytes.hex() # Convert bytes to hex string
                                    index_data[r_id] = pos
                                else:
                                    logger.warning(f"Incomplete index entry found. Skipping remaining index file.")
                                    break
                    except Exception as e:
                        logger.warning(f"Error reading existing index file {self.index_file}: {e}. Index will be rebuilt.")
                        index_data = {} # If corrupted, start fresh with empty index

                index_data[record_id] = position # Update or add the new entry

                with open(self.index_file, 'wb') as f_idx:
                    for r_id, pos in index_data.items():
                        f_idx.write(struct.pack("<32s Q", bytes.fromhex(r_id), pos))
            logger.debug(f"Index entry updated: ID {record_id}, Position {position}")
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for index file {self.lock_file} to write index entry.")
            raise
        except Exception as e:
            logger.error(f"Error writing to index file {self.index_file}: {e}")
            raise

    def rebuild_index(self):
        """
        Rebuilds the index file from scratch, including only valid records.
        This should be called after deletions or if the index file is suspected to be corrupted.
        """
        logger.info("Rebuilding index file...")
        new_index_data = {}
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'rb') as f_db:
                    f_db.seek(0, os.SEEK_END)
                    file_size = f_db.tell()
                    f_db.seek(0, os.SEEK_SET)
                    
                    while f_db.tell() < file_size:
                        start_pos = f_db.tell()
                        header_bytes = f_db.read(RECORD_HEADER_SIZE)
                        if not header_bytes:
                            break # End of file

                        if len(header_bytes) < RECORD_HEADER_SIZE:
                            logger.warning(f"Incomplete header at position {start_pos}. Skipping remaining file.")
                            break

                        record_id_bytes, timestamp_float, data_hash_bytes, is_valid = struct.unpack(RECORD_FORMAT, header_bytes)
                        
                        try:
                            data_size_bytes = f_db.read(4) # Read size of data (4 bytes for int)
                            if len(data_size_bytes) < 4:
                                logger.warning(f"Incomplete data size at position {f_db.tell()}. Skipping remaining file.")
                                break
                            data_size = struct.unpack("<I", data_size_bytes)[0] # Unsigned int
                        except struct.error:
                            logger.warning(f"Could not unpack data size at position {f_db.tell()}. Corrupted record. Skipping.")
                            break

                        f_db.seek(data_size, os.SEEK_CUR) # Skip data content

                        if is_valid:
                            record_id = record_id_bytes.hex()
                            new_index_data[record_id] = start_pos
                
                with open(self.index_file, 'wb') as f_idx:
                    for r_id, pos in new_index_data.items():
                        f_idx.write(struct.pack("<32s Q", bytes.fromhex(r_id), pos))
            logger.info("Index file rebuilt successfully.")
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to rebuild index.")
            raise
        except Exception as e:
            logger.error(f"Error rebuilding index: {e}")
            raise

    def get_record_position_from_index(self, record_id: str) -> Optional[int]:
        """Retrieves the position of a record from the index."""
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                if not os.path.exists(self.index_file):
                    return None
                with open(self.index_file, 'rb') as f_idx:
                    entry_size = struct.calcsize("<32s Q")
                    while True:
                        chunk = f_idx.read(entry_size)
                        if not chunk:
                            break
                        if len(chunk) == entry_size:
                            r_id_bytes, pos = struct.unpack("<32s Q", chunk)
                            if r_id_bytes.hex() == record_id:
                                return pos
                        else:
                            logger.warning(f"Incomplete index entry while searching for {record_id}. Index might be corrupted.")
                            break
            return None
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for index file {self.lock_file} to read record position.")
            return None
        except Exception as e:
            logger.error(f"Error reading from index file {self.index_file}: {e}")
            return None

    def _insert_record(self, record_data: Dict[str, Any], is_valid: bool = True) -> (str, int):
        """
        Internal method to insert a record into the .db file.
        Returns a tuple of (record_id, position_in_db).
        """
        record_id = self._generate_record_id(record_data)
        timestamp = datetime.now().timestamp()
        data_hash = self._calculate_data_hash(record_data)
        data_bytes = json.dumps(record_data).encode('utf-8')
        data_size = len(data_bytes)

        record_header = struct.pack(
            RECORD_FORMAT,
            bytes.fromhex(record_id),
            timestamp,
            bytes.fromhex(data_hash),
            is_valid
        )

        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'ab') as f:
                    current_position = f.tell() # Get current position before writing
                    f.write(record_header)
                    f.write(struct.pack("<I", data_size)) # Write data size as unsigned int
                    f.write(data_bytes)
            
            # Index update will be handled by the caller (insert_data or update_record)
            # as it needs to know if the record ID is new or an update to an existing one.
            logger.info(f"Record {record_id} inserted at position {current_position}.")
            return record_id, current_position
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to insert record.")
            raise
        except IOError as e:
            logger.error(f"IOError inserting record: {e}")
            raise
        except Exception as e:
            logger.error(f"Error inserting record: {traceback.format_exc()}")
            raise

    def insert_data(self, data: Dict[str, Any]) -> str:
        """Public method to insert new accident data."""
        record_id, position = self._insert_record(data, is_valid=True)
        self._write_index_entry(record_id, position) # Update index after insertion
        return record_id

    def _read_record_at_position(self, position: int) -> Optional[Dict[str, Any]]:
        """Reads a single record from a specific byte position."""
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'rb') as f:
                    f.seek(position)
                    header_bytes = f.read(RECORD_HEADER_SIZE)
                    if not header_bytes or len(header_bytes) < RECORD_HEADER_SIZE:
                        logger.warning(f"Incomplete header at position {position}.")
                        return None
                    
                    record_id_bytes, timestamp_float, data_hash_bytes, is_valid = struct.unpack(RECORD_FORMAT, header_bytes)
                    
                    data_size_bytes = f.read(4)
                    if not data_size_bytes or len(data_size_bytes) < 4:
                        logger.warning(f"Incomplete data size at position {f.tell()}.")
                        return None
                    data_size = struct.unpack("<I", data_size_bytes)[0]

                    data_bytes = f.read(data_size)
                    if not data_bytes or len(data_bytes) < data_size:
                        logger.warning(f"Incomplete data at position {f.tell()}. Expected {data_size} bytes, got {len(data_bytes)}.")
                        return None

                    data = json.loads(data_bytes.decode('utf-8'))
                    timestamp = datetime.fromtimestamp(timestamp_float)
                    record_id = record_id_bytes.hex()
                    data_hash = data_hash_bytes.hex()

                    return {
                        "record_id": record_id,
                        "timestamp": timestamp,
                        "data_hash": data_hash,
                        "is_valid": is_valid,
                        "data": data,
                        "position": position # Add position for reference
                    }
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to read record at position {position}.")
            return None
        except Exception as e:
            logger.error(f"Error reading record at position {position}: {traceback.format_exc()}")
            return None

    def get_all_records(self, include_invalid: bool = False) -> Iterator[Dict[str, Any]]:
        """Yields all records from the database, optionally including invalid ones."""
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'rb') as f:
                    f.seek(0, os.SEEK_END)
                    file_size = f.tell()
                    f.seek(0, os.SEEK_SET)

                    while f.tell() < file_size:
                        start_pos = f.tell()
                        header_bytes = f.read(RECORD_HEADER_SIZE)
                        if not header_bytes:
                            break # End of file

                        if len(header_bytes) < RECORD_HEADER_SIZE:
                            logger.warning(f"Incomplete header at position {start_pos}. Skipping remaining file.")
                            break

                        record_id_bytes, timestamp_float, data_hash_bytes, is_valid = struct.unpack(RECORD_FORMAT, header_bytes)
                        
                        try:
                            data_size_bytes = f.read(4) # Read size of data (4 bytes for int)
                            if len(data_size_bytes) < 4:
                                logger.warning(f"Incomplete data size at position {f.tell()}. Skipping remaining file.")
                                break
                            data_size = struct.unpack("<I", data_size_bytes)[0] # Unsigned int
                        except struct.error:
                            logger.warning(f"Could not unpack data size at position {f.tell()}. Corrupted record. Skipping.")
                            break

                        data_bytes = f.read(data_size)
                        if len(data_bytes) < data_size:
                            logger.warning(f"Incomplete data at position {f.tell()}. Expected {data_size} bytes, got {len(data_bytes)}. Skipping.")
                            break

                        try:
                            data = json.loads(data_bytes.decode('utf-8'))
                        except json.JSONDecodeError:
                            logger.error(f"JSON decode error at position {start_pos}. Skipping record.")
                            continue

                        timestamp = datetime.fromtimestamp(timestamp_float)
                        record_id = record_id_bytes.hex()
                        data_hash = data_hash_bytes.hex()

                        if is_valid or include_invalid:
                            yield {
                                "record_id": record_id,
                                "timestamp": timestamp,
                                "data_hash": data_hash,
                                "is_valid": is_valid,
                                "data": data,
                                "position": start_pos
                            }
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to read all records.")
            return
        except Exception as e:
            logger.error(f"Error reading all records: {traceback.format_exc()}")
            return

    def get_record_by_id(self, record_id: str) -> Optional[Dict[str, Any]]:
        """Retrieves a single record by its ID using the index."""
        position = self.get_record_position_from_index(record_id)
        if position is not None:
            record = self._read_record_at_position(position)
            # IMPORTANT: We need to verify that the record found at the indexed position
            # actually has the requested record_id and is valid. The index might be stale
            # between a record being marked invalid and a rebuild_index occurring.
            if record and record['is_valid'] and record['record_id'] == record_id:
                return record
            elif record and not record['is_valid']:
                logger.info(f"Record {record_id} found in index at position {position} but is marked as invalid. Will attempt full scan if not found via index.")
                # Fallback to full scan if indexed record is invalid, in case rebuild_index hasn't run yet
                # (though this specific method is called *after* rebuild_index on delete, so less likely)
                # For update, the new record will have a new ID, so the old one will be invalid.
                # If we want to retrieve the *latest valid version* by an *original* ID,
                # the ID generation strategy needs to be constant, or we need a different index.
                # Given current ID generation (content hash), a content change means new ID.
                # So, this method relies on exact ID match from index.
                pass 
            elif record and record['record_id'] != record_id:
                logger.warning(f"Index points to a record with a different ID. Expected {record_id}, got {record['record_id']} at position {position}. Rebuilding index may be needed.")
            else:
                logger.warning(f"Record {record_id} not found at indexed position {position}.")
        else:
            logger.info(f"Record {record_id} not found in index.")
        
        # Fallback to full scan for safety if index is off or record is not found/invalid in index
        # This can be slow for large DBs, but provides robustness.
        # For a content-based ID, this isn't strictly necessary if 'get_record_by_id'
        # *always* implies finding the exact hash. If 'record_id' is a stable user-facing ID,
        # then the index needs to store a mapping of stable_ID -> content_hash_ID -> position.
        # Sticking with content-hash ID for simplicity here.
        for record in self.get_all_records(include_invalid=False):
            if record['record_id'] == record_id:
                logger.info(f"Record {record_id} found via full scan (index might be stale).")
                return record
        
        return None

    def update_record(self, record_id: str, new_data: Dict[str, Any]) -> bool:
        """
        Updates an existing record by marking the old one as invalid and inserting a new one.
        The index will be updated to point the old record_id to the new position
        IF the new record_id is the same as the old one (which it won't be if content changes).
        More precisely, the logic here is:
        1. Mark old record invalid.
        2. Insert new record (will have a new ID if data changed).
        3. Update the index for the *new* record ID to its new position.
        The old record ID will eventually be cleaned from the index by `rebuild_index`.

        To specifically update the index for the *original* record_id to point to the new position
        (even if data hash changes), you would need to use a stable, non-content-derived record_id
        (e.g., UUIDs for records, where `record_id` in the header is a true UUID).
        For the current content-based ID generation:
        An update effectively creates a new record. We will add the *new* record_id and its position
        to the index. The old record_id will remain in the index pointing to an invalid entry until
        `rebuild_index` is called.
        """
        old_record = self.get_record_by_id(record_id)
        if not old_record or not old_record['is_valid']:
            st.error(f"Record with ID {record_id} not found or is invalid for update.")
            logger.warning(f"Attempted to update non-existent or invalid record {record_id}")
            return False

        # Check if data actually changed
        if old_record['data'] == new_data:
            st.info(f"No changes detected for record ID {record_id}.")
            logger.info(f"Update called for record {record_id} but data is identical.")
            return True

        # Mark the old record as invalid
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'r+b') as f:
                    f.seek(old_record['position'])
                    
                    # Read original header to preserve parts of it
                    original_header_bytes = f.read(RECORD_HEADER_SIZE)
                    if len(original_header_bytes) < RECORD_HEADER_SIZE:
                        logger.error(f"Failed to read complete header for update at position {old_record['position']}.")
                        return False
                    
                    record_id_bytes, timestamp_float, data_hash_bytes, _ = struct.unpack(RECORD_FORMAT, original_header_bytes)
                    
                    # Pack new header with is_valid=False
                    updated_header = struct.pack(
                        RECORD_FORMAT,
                        record_id_bytes,      # Keep original ID
                        timestamp_float,      # Keep original timestamp
                        data_hash_bytes,      # Keep original data hash (though it's for the old data)
                        False                 # Mark as invalid
                    )
                    
                    f.seek(old_record['position']) # Seek back to overwrite
                    f.write(updated_header)
            logger.info(f"Record {record_id} at position {old_record['position']} marked as invalid.")

            # Insert the new record with the updated data
            # This new record will have a new ID because its content (new_data) is different.
            new_record_id, new_position = self._insert_record(new_data, is_valid=True)
            
            # --- OPTIMIZATION START ---
            # Update the index for the NEW record_id to point to its new position.
            # This ensures that any future lookup for the *new* record_id will go to the correct place.
            # The *old* record_id will still point to the invalid entry in the index until `rebuild_index` is called.
            # If the user still tries to find the old record_id, `get_record_by_id` might find the invalid entry
            # and then perform a full scan (as implemented as a fallback).
            # The most robust way for 'update' to work with content-based IDs is that 'update' means 'replace'.
            # And 'get_record_by_id' should always search for the *latest valid* record with that ID,
            # or specifically query for the content-based hash.
            #
            # Given the current ID generation (content-based hash), an update *will* result in a new ID.
            # So, the index for the *new* ID needs to be written. The old one becomes stale.
            self._write_index_entry(new_record_id, new_position)
            logger.info(f"Index for new record ID {new_record_id} updated to position {new_position}.")
            # --- OPTIMIZATION END ---


            if new_record_id:
                st.success(f"Registro {record_id} atualizado com sucesso. Novo registro válido ID: `{new_record_id}`")
                return True
            else:
                st.error(f"Falha ao inserir nova versão do registro {record_id}.")
                return False

        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to update record.")
            return False
        except IOError as e:
            logger.error(f"IOError updating record {record_id}: {e}")
            return False
        except Exception as e:
            logger.error(f"Error updating record {record_id}: {traceback.format_exc()}")
            return False

    def delete_record(self, record_id: str) -> bool:
        """
        Deletes a record by marking it as invalid.
        Then, rebuilds the index.
        """
        record_to_delete = self.get_record_by_id(record_id)
        if not record_to_delete or not record_to_delete['is_valid']:
            st.error(f"Record with ID {record_id} not found or already invalid for deletion.")
            logger.warning(f"Attempted to delete non-existent or invalid record {record_id}")
            return False

        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'r+b') as f:
                    f.seek(record_to_delete['position'])
                    
                    original_header_bytes = f.read(RECORD_HEADER_SIZE)
                    if len(original_header_bytes) < RECORD_HEADER_SIZE:
                        logger.error(f"Failed to read complete header for deletion at position {record_to_delete['position']}.")
                        return False
                    
                    record_id_bytes, timestamp_float, data_hash_bytes, _ = struct.unpack(RECORD_FORMAT, original_header_bytes)
                    
                    updated_header = struct.pack(
                        RECORD_FORMAT,
                        record_id_bytes,
                        timestamp_float,
                        data_hash_bytes,
                        False # Mark as invalid
                    )
                    
                    f.seek(record_to_delete['position']) # Seek back to overwrite
                    f.write(updated_header)
            logger.info(f"Record {record_id} at position {record_to_delete['position']} marked as invalid.")
            st.success(f"Registro {record_id} marcado como excluído com sucesso.")

            # Rebuild index after deletion
            # This will remove the invalid record_id from the index.
            self.rebuild_index()
            return True

        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to delete record.")
            return False
        except IOError as e:
            logger.error(f"IOError deleting record {record_id}: {e}")
            return False
        except Exception as e:
            logger.error(f"Error deleting record {record_id}: {traceback.format_exc()}")
            return False

    def _get_next_record_position(self) -> int:
        """Returns the byte offset for the next record to be written."""
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'ab') as f: # Open in append binary mode
                    return f.tell()
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to get next record position.")
            raise
        except IOError as e:
            logger.error(f"IOError getting next record position: {e}")
            raise
        except Exception as e:
            logger.error(f"Error getting next record position: {traceback.format_exc()}")
            raise

    def get_number_of_records(self) -> int:
        """Returns the total number of valid records in the database."""
        count = 0
        try:
            for record in self.get_all_records(include_invalid=False):
                count += 1
        except Exception as e:
            logger.error(f"Error counting records: {e}")
        return count

    def search_records(self, query: str) -> List[Dict[str, Any]]:
        """
        Searches for records where any string value in the data contains the query.
        Case-insensitive search.
        """
        results = []
        try:
            for record in self.get_all_records():
                if not record['is_valid']:
                    continue
                
                # Search within the 'data' dictionary values
                found = False
                for key, value in record['data'].items():
                    if isinstance(value, str) and query.lower() in value.lower():
                        found = True
                        break
                    elif isinstance(value, (int, float)) and query.lower() in str(value).lower():
                        found = True
                        break
                
                if found:
                    results.append(record)
        except Exception as e:
            logger.error(f"Error during search: {e}")
        return results

    def import_from_csv(self, file_path: str) -> int:
        """Imports data from a CSV file into the database."""
        imported_count = 0
        try:
            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
            if file_size_mb > MAX_FILE_SIZE_MB:
                st.error(f"File size exceeds maximum allowed ({MAX_FILE_SIZE_MB}MB).")
                logger.warning(f"Import failed: File {file_path} too large.")
                return 0

            with open(file_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile, delimiter=CSV_DELIMITER)
                for row in reader:
                    # Clean up keys (remove BOM if present)
                    cleaned_row = {k.strip().replace('\ufeff', ''): v for k, v in row.items()}
                    
                    # Convert specific fields to appropriate types if necessary
                    # Example: 'Data' to date object, 'Hora' to time string, numeric fields to int/float
                    processed_row = {}
                    for key, value in cleaned_row.items():
                        if key.lower() == 'data' and value:
                            try:
                                processed_row[key] = datetime.strptime(value, '%Y-%m-%d').date().isoformat()
                            except ValueError:
                                logger.warning(f"Could not parse date '{value}' for field '{key}'. Keeping as string.")
                                processed_row[key] = value
                        elif key.lower() in ['numero', 'idade', 'quantidade'] and value: # Example numeric fields
                            try:
                                processed_row[key] = int(value)
                            except ValueError:
                                try:
                                    processed_row[key] = float(value)
                                except ValueError:
                                    processed_row[key] = value # Keep as string if not number
                        else:
                            processed_row[key] = value

                    # Use _insert_record to get position, then update index
                    record_id, position = self._insert_record(processed_row)
                    self._write_index_entry(record_id, position)
                    imported_count += 1
            logger.info(f"Successfully imported {imported_count} records from {file_path}.")
            st.success(f"Imported {imported_count} records successfully.")
        except FileNotFoundError:
            st.error(f"CSV file not found at {file_path}")
            logger.error(f"CSV import failed: File not found {file_path}")
        except Exception as e:
            st.error(f"An error occurred during CSV import: {e}")
            logger.error(f"Error during CSV import from {file_path}: {traceback.format_exc()}")
        return imported_count

    def export_to_csv(self, output_file: str) -> bool:
        """Exports all valid records to a CSV file."""
        try:
            records = list(self.get_all_records()) # Get all records in memory for CSV writer
            if not records:
                st.info("No records to export.")
                return False

            # Collect all unique keys to use as CSV headers
            all_keys = set()
            for record in records:
                all_keys.update(record['data'].keys())
            
            # Sort keys for consistent header order
            fieldnames = sorted(list(all_keys))

            with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=CSV_DELIMITER)
                writer.writeheader()
                for record in records:
                    writer.writerow(record['data'])
            logger.info(f"Successfully exported {len(records)} records to {output_file}.")
            st.success(f"Exported {len(records)} records to `{output_file}` successfully.")
            return True
        except Exception as e:
            st.error(f"An error occurred during CSV export: {e}")
            logger.error(f"Error during CSV export to {output_file}: {traceback.format_exc()}")
            return False

    def create_backup(self) -> Optional[str]:
        """Creates a timestamped backup of the database file."""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = os.path.join(BACKUP_DIR, f"traffic_accidents_backup_{timestamp}.db")
            
            with filelock.FileLock(self.lock_file, timeout=10):
                # Ensure backup directory exists
                os.makedirs(BACKUP_DIR, exist_ok=True)
                
                # Copy the database file
                with open(self.db_file, 'rb') as src, open(backup_file, 'wb') as dst:
                    while True:
                        chunk = src.read(CHUNK_SIZE)
                        if not chunk:
                            break
                        dst.write(chunk)
                
                # Also backup the index file
                backup_index_file = os.path.join(BACKUP_DIR, f"index_backup_{timestamp}.idx")
                if os.path.exists(self.index_file):
                    with open(self.index_file, 'rb') as src_idx, open(backup_index_file, 'wb') as dst_idx:
                        while True:
                            chunk = src_idx.read(CHUNK_SIZE)
                            if not chunk:
                                break
                            dst_idx.write(chunk)
                else:
                    logger.warning(f"Index file {self.index_file} not found for backup.")

            logger.info(f"Database and index backed up to {backup_file} and {backup_index_file}")
            self._manage_backups()
            return backup_file
        except filelock.Timeout:
            st.error(f"Could not acquire lock for database file {self.lock_file} to create backup.")
            logger.error(f"Backup failed: Lock timeout.")
            return None
        except Exception as e:
            st.error(f"Error creating backup: {e}")
            logger.error(f"Error creating backup: {traceback.format_exc()}")
            return None

    def _manage_backups(self):
        """Deletes old backups, keeping only the most recent N."""
        try:
            backups = sorted(
                [os.path.join(BACKUP_DIR, f) for f in os.listdir(BACKUP_DIR) if f.startswith('traffic_accidents_backup_') and f.endswith('.db')],
                key=os.path.getmtime,
                reverse=True
            )
            
            for i in range(MAX_BACKUPS, len(backups)):
                old_db_backup = backups[i]
                os.remove(old_db_backup)
                logger.info(f"Deleted old database backup: {old_db_backup}")
                
                # Also delete corresponding index backup
                base_name = os.path.basename(old_db_backup).replace('traffic_accidents_backup_', 'index_backup_').replace('.db', '.idx')
                old_index_backup = os.path.join(BACKUP_DIR, base_name)
                if os.path.exists(old_index_backup):
                    os.remove(old_index_backup)
                    logger.info(f"Deleted old index backup: {old_index_backup}")

        except Exception as e:
            logger.error(f"Error managing backups: {e}")

# --- Streamlit UI Setup ---
def setup_ui():
    st.set_page_config(page_title="Traffic Accidents Database", layout="wide")
    st.title("Sistema de Gestão de Dados de Acidentes de Trânsito")

    db = TrafficAccidentsDB(DB_FILE, INDEX_FILE, LOCK_FILE)

    # Tabs for navigation
    tab_insert, tab_view, tab_search, tab_import_export, tab_admin = st.tabs([
        "Inserir Registro", "Visualizar Registros", "Buscar Registros", "Importar/Exportar", "Administração"
    ])

    with tab_insert:
        st.header("Inserir Novo Registro")
        with st.form("insert_form"):
            # Example fields for a traffic accident
            data_acidente = st.date_input("Data do Acidente", datetime.now().date())
            hora_acidente = st.time_input("Hora do Acidente", datetime.now().time())
            local = st.text_input("Local (Ex: Rua A, Cruzamento com B)")
            tipo_acidente = st.selectbox("Tipo de Acidente", ["Colisão", "Atropelamento", "Capotamento", "Saída de Pista", "Outro"])
            veiculos_envolvidos = st.number_input("Número de Veículos Envolvidos", min_value=1, value=1)
            vitimas = st.number_input("Número de Vítimas", min_value=0, value=0)
            fatalidades = st.number_input("Número de Fatalidades", min_value=0, value=0)
            descricao = st.text_area("Descrição Breve")

            submitted = st.form_submit_button("Inserir Registro")
            if submitted:
                record_data = {
                    "Data": data_acidente.isoformat(), # Store date as ISO format string
                    "Hora": str(hora_acidente),
                    "Local": local,
                    "Tipo": tipo_acidente,
                    "Veiculos_Envolvidos": veiculos_envolvidos,
                    "Vitimas": vitimas,
                    "Fatalidades": fatalidades,
                    "Descricao": descricao
                }
                try:
                    record_id = db.insert_data(record_data)
                    st.success(f"Registro inserido com sucesso! ID do Registro: `{record_id}`")
                    logger.info(f"New record inserted via UI: {record_id}")
                except Exception as e:
                    st.error(f"Erro ao inserir registro: {e}")
                    logger.error(f"UI insert error: {traceback.format_exc()}")

    with tab_view:
        st.header("Visualizar e Gerenciar Registros")
        
        records_per_page = st.slider("Registros por página", 10, 50, MAX_RECORDS_PER_PAGE)
        
        all_records = list(db.get_all_records(include_invalid=False))
        total_records = len(all_records)
        total_pages = (total_records + records_per_page - 1) // records_per_page

        if 'current_page' not in st.session_state:
            st.session_state.current_page = 1

        col1, col2, col3 = st.columns([1,2,1])
        with col1:
            if st.button("Página Anterior"):
                if st.session_state.current_page > 1:
                    st.session_state.current_page -= 1
        with col2:
            st.write(f"Página {st.session_state.current_page} de {total_pages}")
        with col3:
            if st.button("Próxima Página"):
                if st.session_state.current_page < total_pages:
                    st.session_state.current_page += 1

        start_idx = (st.session_state.current_page - 1) * records_per_page
        end_idx = start_idx + records_per_page
        displayed_records = all_records[start_idx:end_idx]

        if displayed_records:
            for record in displayed_records:
                expander_title = f"ID: {record['record_id'][:8]}... - Data: {record['data'].get('Data', 'N/A')} - Local: {record['data'].get('Local', 'N/A')}"
                with st.expander(expander_title):
                    st.json(record['data'])
                    st.write(f"**ID do Registro:** `{record['record_id']}`")
                    st.write(f"**Timestamp de Criação/Atualização:** {record['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                    st.write(f"**Hash dos Dados:** `{record['data_hash']}`")
                    st.write(f"**Válido:** {'Sim' if record['is_valid'] else 'Não'}")
                    st.write(f"**Posição no DB:** `{record['position']}`")

                    col_edit, col_delete = st.columns(2)
                    with col_edit:
                        if st.button(f"Editar {record['record_id']}", key=f"edit_{record['record_id']}"):
                            st.session_state.edit_record_id = record['record_id']
                            st.session_state.edit_record_data = record['data']
                            st.session_state.show_edit_form = True
                            st.experimental_rerun() # Rerun to show edit form immediately
                    with col_delete:
                        if st.button(f"Excluir {record['record_id']}", key=f"delete_{record['record_id']}"):
                            # Add a confirmation step for deletion
                            if st.warning(f"Tem certeza que deseja excluir o registro {record['record_id']}? Isso o marcará como inválido e reconstruirá o índice."):
                                if st.button("Confirmar Exclusão", key=f"confirm_delete_{record['record_id']}"):
                                    try:
                                        if db.delete_record(record['record_id']):
                                            st.success("Registro excluído com sucesso (marcado como inválido).")
                                            logger.info(f"Record {record['record_id']} deleted via UI.")
                                            time.sleep(1) # Give time for message to display
                                            st.experimental_rerun()
                                        else:
                                            st.error("Falha ao excluir registro.")
                                    except Exception as e:
                                        st.error(f"Erro ao excluir registro: {e}")
                                        logger.error(f"UI delete error: {traceback.format_exc()}")
        else:
            st.info("Nenhum registro encontrado.")

        # Edit Form Logic
        if st.session_state.get('show_edit_form', False) and st.session_state.get('edit_record_id'):
            st.header(f"Editar Registro: {st.session_state.edit_record_id[:8]}...")
            record_to_edit_data = st.session_state.edit_record_data

            with st.form("edit_record_form"):
                # Pre-fill fields with existing data
                edited_data_acidente = st.date_input("Data do Acidente", 
                                                    value=date.fromisoformat(record_to_edit_data.get('Data')) if record_to_edit_data.get('Data') else datetime.now().date(),
                                                    key="edit_data_acidente")
                
                # Handling time input requires a bit more care if it's stored as string and not datetime.time object
                # For simplicity, convert back and forth
                edited_hora_acidente_str = record_to_edit_data.get('Hora', '00:00:00')
                try:
                    edited_hora_acidente = datetime.strptime(edited_hora_acidente_str, '%H:%M:%S').time()
                except ValueError:
                    edited_hora_acidente = datetime.now().time() # Default if parsing fails
                
                edited_hora_acidente = st.time_input("Hora do Acidente", value=edited_hora_acidente, key="edit_hora_acidente")

                edited_local = st.text_input("Local (Ex: Rua A, Cruzamento com B)", value=record_to_edit_data.get('Local', ''), key="edit_local")
                edited_tipo_acidente = st.selectbox("Tipo de Acidente", ["Colisão", "Atropelamento", "Capotamento", "Saída de Pista", "Outro"], 
                                                    index=["Colisão", "Atropelamento", "Capotamento", "Saída de Pista", "Outro"].index(record_to_edit_data.get('Tipo', 'Colisão')), 
                                                    key="edit_tipo_acidente")
                edited_veiculos_envolvidos = st.number_input("Número de Veículos Envolvidos", min_value=1, value=record_to_edit_data.get('Veiculos_Envolvidos', 1), key="edit_veiculos_envolvidos")
                edited_vitimas = st.number_input("Número de Vítimas", min_value=0, value=record_to_edit_data.get('Vitimas', 0), key="edit_vitimas")
                edited_fatalidades = st.number_input("Número de Fatalidades", min_value=0, value=record_to_edit_data.get('Fatalidades', 0), key="edit_fatalidades")
                edited_descricao = st.text_area("Descrição Breve", value=record_to_edit_data.get('Descricao', ''), key="edit_descricao")

                update_submitted = st.form_submit_button("Atualizar Registro")
                cancel_update = st.form_submit_button("Cancelar")

                if update_submitted:
                    new_record_data = {
                        "Data": edited_data_acidente.isoformat(),
                        "Hora": str(edited_hora_acidente),
                        "Local": edited_local,
                        "Tipo": edited_tipo_acidente,
                        "Veiculos_Envolvidos": edited_veiculos_envolvidos,
                        "Vitimas": edited_vitimas,
                        "Fatalidades": edited_fatalidades,
                        "Descricao": edited_descricao
                    }
                    try:
                        if db.update_record(st.session_state.edit_record_id, new_record_data):
                            st.success("Registro atualizado com sucesso!")
                            logger.info(f"Record {st.session_state.edit_record_id} updated via UI.")
                            st.session_state.show_edit_form = False
                            st.session_state.edit_record_id = None
                            st.session_state.edit_record_data = None
                            time.sleep(1) # Give time for message to display
                            st.experimental_rerun()
                        else:
                            st.error("Falha ao atualizar registro.")
                    except Exception as e:
                        st.error(f"Erro ao atualizar registro: {e}")
                        logger.error(f"UI update error: {traceback.format_exc()}")
                
                if cancel_update:
                    st.session_state.show_edit_form = False
                    st.session_state.edit_record_id = None
                    st.session_state.edit_record_data = None
                    st.experimental_rerun()


    with tab_search:
        st.header("Buscar Registros")
        search_query = st.text_input("Digite sua busca (ex: nome da rua, tipo de acidente)")
        if st.button("Buscar"):
            if search_query:
                found_records = db.search_records(search_query)
                if found_records:
                    st.subheader(f"Resultados da Busca para '{search_query}':")
                    for record in found_records:
                        with st.expander(f"ID: {record['record_id'][:8]}... - Data: {record['data'].get('Data', 'N/A')} - Local: {record['data'].get('Local', 'N/A')}"):
                            st.json(record['data'])
                            st.write(f"**ID do Registro:** `{record['record_id']}`")
                            st.write(f"**Timestamp de Criação/Atualização:** {record['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                            st.write(f"**Hash dos Dados:** `{record['data_hash']}`")
                            st.write(f"**Válido:** {'Sim' if record['is_valid'] else 'Não'}")
                            st.write(f"**Posição no DB:** `{record['position']}`")
                else:
                    st.info("Nenhum registro encontrado para sua busca.")
            else:
                st.warning("Por favor, digite um termo para buscar.")

    with tab_import_export:
        st.header("Importar e Exportar Dados")

        st.subheader("Importar CSV")
        uploaded_file = st.file_uploader("Escolha um arquivo CSV", type=["csv"])
        if uploaded_file is not None:
            # Save uploaded file to a temporary location
            with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                temp_csv_path = tmp_file.name
            
            if st.button("Iniciar Importação"):
                try:
                    imported_count = db.import_from_csv(temp_csv_path)
                    st.success(f"Importação concluída. {imported_count} registros importados.")
                    logger.info(f"CSV imported from {temp_csv_path}, {imported_count} records.")
                except Exception as e:
                    st.error(f"Erro durante a importação: {e}")
                    logger.error(f"CSV import failed for {temp_csv_path}: {traceback.format_exc()}")
                finally:
                    os.unlink(temp_csv_path) # Clean up temp file

        st.subheader("Exportar para CSV")
        export_file_name = st.text_input("Nome do arquivo para exportar (ex: acidentes.csv)", "traffic_accidents_export.csv")
        export_path = os.path.join(DB_DIR, export_file_name)
        if st.button("Exportar Dados"):
            try:
                if db.export_to_csv(export_path):
                    with open(export_path, "rb") as file_to_download:
                        st.download_button(
                            label="Baixar Arquivo CSV",
                            data=file_to_download.read(),
                            file_name=export_file_name,
                            mime="text/csv"
                        )
                    logger.info(f"Data exported to {export_path} and offered for download.")
            except Exception as e:
                st.error(f"Erro ao preparar exportação: {e}")
                logger.error(f"Export preparation failed for {export_path}: {traceback.format_exc()}")

    with tab_admin:
        st.header("Ferramentas de Administração")

        st.subheader("Criar Backup do Banco de Dados")
        if st.button("Criar Backup Agora"):
            try:
                backup_path = db.create_backup()
                if backup_path:
                    st.success(f"Backup criado com sucesso em `{backup_path}`")
                    logger.info(f"Manual backup created: {backup_path}")
                else:
                    st.error("Falha ao criar backup.")
            except Exception as e:
                st.error(f"Erro ao criar backup: {e}")
                logger.error(f"Backup creation error: {traceback.format_exc()}")
        
        st.subheader("Reconstruir Arquivo de Índice")
        st.info("Isto irá recriar o arquivo de índice (`index.idx`) com base apenas nos registros válidos no banco de dados. Útil para corrigir inconsistências ou após exclusões lógicas.")
        if st.button("Reconstruir Índice"):
            try:
                db.rebuild_index()
                st.success("Índice reconstruído com sucesso!")
                logger.info("Index rebuilt successfully via UI.")
            except Exception as e:
                st.error(f"Erro ao reconstruir índice: {e}")
                logger.error(f"Index rebuild error: {traceback.format_exc()}")

        st.subheader("Log de Atividades Recentes")
        st.write("Exibindo as últimas 10 entradas do log de atividades (mais recentes primeiro):")
        try:
            log_file_path = 'traffic_accidents.log'
            if os.path.exists(log_file_path):
                with open(log_file_path, 'r', encoding='utf-8') as f:
                    # Read all lines and reverse to get most recent first
                    log_lines = f.readlines()
                    log_lines.reverse() # Get most recent first
                    
                    update_entries = []
                    for line in log_lines:
                        # Simple parsing to find relevant log entries
                        if "inserted" in line or "updated" in line or "deleted" in line or "imported" in line or "rebuilt" in line:
                            try:
                                # Extract timestamp and message
                                parts = line.split(' - ', 3)
                                if len(parts) >= 4:
                                    timestamp_str = parts[0]
                                    message = parts[3].strip()
                                    update_entries.append(f"**`{timestamp_str}`** `{message}`")
                                    if len(update_entries) >= MAX_LOG_ENTRIES_DISPLAY:
                                        break
                            except Exception as e:
                                logger.warning(f"Failed to parse log line for registry: {line.strip()} - {e}")
                                continue
                
                if update_entries:
                    for entry in update_entries:
                        st.markdown(entry)
                else:
                    st.info("No recent record updates or imports found in the log.")
            else:
                st.info("Arquivo de log de atividades não encontrado.")
        except Exception as e:
            st.error(f"Could not read activity log: {str(e)}")
            logger.error(f"Error reading activity log: {traceback.format_exc()}")

# --- Main Application Entry Point ---
if __name__ == "__main__":
    # Ensure the DB directory is created before any DB operations
    # This is done within TrafficAccidentsDB.__init__ but can be called standalone
    # to catch early errors if __init__ fails before setup_ui handles it.
    try:
        os.makedirs(DB_DIR, exist_ok=True)
        os.makedirs(BACKUP_DIR, exist_ok=True)
    except OSError as e:
        st.error(f"Critical: Cannot create database directories. Please check permissions for {DB_DIR}. Error: {e}")
        logger.critical(f"Initial directory creation failed: {traceback.format_exc()}")
        st.stop() # Stop the app if directories cannot be created

    setup_ui() # Start the Streamlit UI