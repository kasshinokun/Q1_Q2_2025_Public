# -*- coding: utf-8 -*-
"""It's possible show me this code on Google Colab?

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L2b0-OyeUosO0NKy8QawlJQPWt0i8qx6
"""

import streamlit as st
import csv
import os
import struct
import json
import hashlib
import time
import filelock
import logging
from datetime import datetime, date
from pathlib import Path
from typing import List, Dict, Optional, Union, Callable, Any, Iterator
import tempfile
import traceback

# --- Configuration Constants (Centralized - Adapted from v4epsilon) ---
APP_CONFIG = {
    "DB_DIR": os.path.join(Path.home(), 'Documents', 'Data'),
    "DB_FILE_NAME": 'traffic_accidents.db',
    "INDEX_FILE_NAME": 'index.idx',
    "LOCK_FILE_NAME": 'traffic_accidents.lock',
    "ID_COUNTER_FILE_NAME": 'id_counter.dat',  # New file for auto-increment ID
    "BACKUP_DIR_NAME": 'backups',
    "CSV_DELIMITER": ';',
    "MAX_RECORDS_PER_PAGE": 20,
    "MAX_FILE_SIZE_MB": 100,
    "CHUNK_SIZE": 4096,
    "MAX_BACKUPS": 5,
    "MAX_LOG_ENTRIES_DISPLAY": 10,
    "LOG_FILE_NAME": 'traffic_accidents.log'
}

# Derived paths
DB_PATH = os.path.join(APP_CONFIG["DB_DIR"], APP_CONFIG["DB_FILE_NAME"])
INDEX_PATH = os.path.join(APP_CONFIG["DB_DIR"], APP_CONFIG["INDEX_FILE_NAME"])
LOCK_PATH = os.path.join(APP_CONFIG["DB_DIR"], APP_CONFIG["LOCK_FILE_NAME"])
ID_COUNTER_PATH = os.path.join(APP_CONFIG["DB_DIR"], APP_CONFIG["ID_COUNTER_FILE_NAME"])
BACKUP_PATH = os.path.join(APP_CONFIG["DB_DIR"], APP_CONFIG["BACKUP_DIR_NAME"])
LOG_FILE_PATH = os.path.join(APP_CONFIG["DB_DIR"], APP_CONFIG["LOG_FILE_NAME"])

# Configure logging (Adapted from v4epsilon)
logging.basicConfig(
    level=logging.logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE_PATH),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- Data Fields (From v3alpha) ---
FIELDS = [
    'crash_date', 'traffic_control_device', 'weather_condition',
    'lighting_condition', 'first_crash_type', 'trafficway_type',
    'alignment', 'roadway_surface_cond', 'road_defect', 'crash_type',
    'intersection_related_i', 'damage', 'prim_contributory_cause',
    'num_units', 'most_severe_injury', 'injuries_total', 'injuries_fatal',
    'injuries_incapacitating', 'injuries_non_incapacitating',
    'injuries_reported_not_evident', 'injuries_no_indication',
    'crash_hour', 'crash_day_of_week', 'crash_month'
]

# --- Custom Exceptions (From v3alpha) ---
class DataValidationError(Exception):
    """Custom exception for data validation errors within DataObject."""
    pass

class DatabaseError(Exception):
    """Custom exception for database operations and file I/O errors."""
    pass

class FileLockError(Exception):
    """Custom exception for errors acquiring/releasing file locks."""
    pass

# --- Utility Functions for String Manipulation ---
def split_phrase_into_array(phrase: str) -> List[str]:
    """
    Reads a phrase and returns it as a list of strings.
    If the phrase contains ',' or '/', it performs a split operation
    and returns the resulting parts as a list of strings.
    Each part is stripped of leading/trailing whitespace.
    """
    if not isinstance(phrase, str):
        return [] # Or raise TypeError if non-string input is not allowed

    if ',' in phrase:
        return [part.strip() for part in phrase.split(',')]
    elif '/' in phrase:
        return [part.strip() for part in phrase.split('/')]
    else:
        return [phrase.strip()]

def join_array_with_character(string_array: List[str], character: str) -> str:
    """
    Receives a list of strings and a character (',' or '/') to return a single string
    where the array elements are joined by the received character, with spaces around it.
    Example: ["apple", "orange"] with character "/" -> "apple / orange"
    Example: ["one", "two", "three"] with character "," -> "one , two , three"
    """
    if not isinstance(string_array, list) or not all(isinstance(s, str) for s in string_array):
        raise TypeError("string_array must be a list of strings.")
    if not isinstance(character, str) or character not in [',', '/']:
        raise ValueError("character must be a string and either ',' or '/'.")

    return (f" {character} ").join(string_array)


# --- DataObject Class (Adapted from v3alpha with to_dict and from_dict, and added 'id') ---
class DataObject:
    """
    Represents a traffic accident record with enhanced validation and serialization.
    Each instance corresponds to a single record in the database.
    Added methods to convert to/from dictionary, and an 'id' field.
    """

    def __init__(self, row_data: Optional[List[str]] = None, existing_data_dict: Optional[Dict[str, Any]] = None):
        """
        Initializes a DataObject.
        Can be initialized from a list of row data (e.g., from CSV) or an existing dictionary.
        """
        self._initialize_defaults()

        if row_data is not None:
            try:
                self._initialize_from_row(row_data)
            except (DataValidationError, ValueError) as e:
                logger.error(f"Error initializing DataObject from row: {str(e)} | Row: {row_data}")
                raise DataValidationError(f"Invalid data for record: {str(e)}")
        elif existing_data_dict is not None:
            try:
                self._initialize_from_dict(existing_data_dict)
            except (DataValidationError, ValueError) as e:
                logger.error(f"Error initializing DataObject from dict: {str(e)} | Dict: {existing_data_dict}")
                raise DataValidationError(f"Invalid data for record: {str(e)}")

        # Final validation after initialization
        if not self.validate():
            raise DataValidationError("Data validation failed after initialization.")

    def _initialize_defaults(self):
        """Initializes all fields with safe and type-appropriate default values."""
        self.id: Optional[int] = None # Added 'id' field
        self.crash_date = ""
        self.traffic_control_device = "UNKNOWN"
        self.weather_condition = "UNKNOWN"
        self.lighting_condition = "UNKNOWN"
        self.first_crash_type = "UNKNOWN"
        self.trafficway_type = "UNKNOWN"
        self.alignment = "UNKNOWN"
        self.roadway_surface_cond = "UNKNOWN"
        self.road_defect = "NONE"
        self.crash_type = "UNKNOWN"
        self.intersection_related_i = "NO"
        self.damage = "UNKNOWN"
        self.prim_contributory_cause = "UNKNOWN"
        self.num_units = 0
        self.most_severe_injury = "NONE"
        self.injuries_total = 0.0
        self.injuries_fatal = 0.0
        self.injuries_incapacitating = 0.0
        self.injuries_non_incapacitating = 0.0
        self.injuries_reported_not_evident = 0.0
        self.injuries_no_indication = 0.0
        self.crash_hour = 0
        self.crash_day_of_week = 1
        self.crash_month = 1

    def _initialize_from_row(self, row_data: List[str]):
        """Populates object fields from a list of strings (e.g., from a CSV row)."""
        if len(row_data) != len(FIELDS):
            raise ValueError(f"Expected {len(FIELDS)} fields, but got {len(row_data)}.")

        processed_data = [value.strip() if isinstance(value, str) else None for value in row_data]

        self.crash_date = self._validate_date(processed_data[0])
        self.traffic_control_device = self._validate_string(processed_data[1], "traffic_control_device", allow_empty=True)
        self.weather_condition = self._validate_string(processed_data[2], "weather_condition", allow_empty=True)
        self.lighting_condition = self._validate_string(processed_data[3], "lighting_condition", allow_empty=True)
        self.first_crash_type = self._validate_string(processed_data[4], "first_crash_type", allow_empty=True)
        self.trafficway_type = self._validate_string(processed_data[5], "trafficway_type", allow_empty=True)
        self.alignment = self._validate_string(processed_data[6], "alignment", allow_empty=True)
        self.roadway_surface_cond = self._validate_string(processed_data[7], "roadway_surface_cond", allow_empty=True)
        self.road_defect = self._validate_string(processed_data[8], "road_defect", allow_empty=True)
        self.crash_type = self._validate_string(processed_data[9], "crash_type", allow_empty=False)
        self.intersection_related_i = self._validate_yes_no(processed_data[10])
        self.damage = self._validate_string(processed_data[11], "damage", allow_empty=True)
        self.prim_contributory_cause = self._validate_string(processed_data[12], "prim_contributory_cause", allow_empty=True)
        self.num_units = self._validate_positive_int(processed_data[13], "num_units", min_val=0)
        self.most_severe_injury = self._validate_string(processed_data[14], "most_severe_injury", allow_empty=True)
        self.injuries_total = self._validate_positive_float(processed_data[15], "injuries_total", min_val=0.0)
        self.injuries_fatal = self._validate_positive_float(processed_data[16], "injuries_fatal", min_val=0.0)
        self.injuries_incapacitating = self._validate_positive_float(processed_data[17], "injuries_incapacitating", min_val=0.0)
        self.injuries_non_incapacitating = self._validate_positive_float(processed_data[18], "injuries_non_incapacitating", min_val=0.0)
        self.injuries_reported_not_evident = self._validate_positive_float(processed_data[19], "injuries_reported_not_evident", min_val=0.0)
        self.injuries_no_indication = self._validate_positive_float(processed_data[20], "injuries_no_indication", min_val=0.0)
        self.crash_hour = self._validate_range(processed_data[21], "crash_hour", 0, 23)
        self.crash_day_of_week = self._validate_range(processed_data[22], "crash_day_of_week", 1, 7)
        self.crash_month = self._validate_range(processed_data[23], "crash_month", 1, 12)

    def _initialize_from_dict(self, data_dict: Dict[str, Any]):
        """Populates object fields from a dictionary (e.g., from JSON deserialization)."""
        # Ensure 'id' is an integer
        retrieved_id = data_dict.get('id', data_dict.get('record_id'))
        if retrieved_id is not None:
            try:
                self.id = int(retrieved_id)
            except (ValueError, TypeError):
                logger.warning(f"Could not convert 'id' ({retrieved_id}) to integer. Setting to None.")
                self.id = None
        else:
            self.id = None

        for field in FIELDS:
            if field in data_dict:
                if field == 'crash_date':
                    self.crash_date = self._validate_date(data_dict[field])
                elif field in ['num_units', 'crash_hour', 'crash_day_of_week', 'crash_month']:
                    setattr(self, field, self._validate_positive_int(str(data_dict[field]), field))
                elif field.startswith('injuries_'):
                    setattr(self, field, self._validate_positive_float(str(data_dict[field]), field))
                elif field == 'intersection_related_i':
                    self.intersection_related_i = self._validate_yes_no(data_dict[field])
                else:
                    setattr(self, field, self._validate_string(data_dict[field], field, allow_empty=True))
            else:
                logger.warning(f"Field '{field}' missing in provided dictionary for DataObject initialization. Using default.")

    @staticmethod
    def _validate_date(date_str: Optional[str]) -> str:
        """Validates and standardizes a date string to APAC-MM-DD format."""
        if not date_str:
            return ""

        date_str = date_str.strip()
        if not date_str:
            return ""

        for fmt in ('%Y-%m-%d', '%m/%d/%Y', '%d-%m-%Y', '%Y/%m/%d',
                    '%m/%d/%Y %H:%M:%S', '%Y-%m-%d %H:%M:%S',
                    '%m/%d/%Y %I:%M:%S %p', '%Y-%m-%d %I:%M:%S %p'):
            try:
                dt = datetime.strptime(date_str, fmt)
                formatted_date = dt.strftime('%Y-%m-%d')
                return formatted_date
            except ValueError:
                continue

        raise DataValidationError(f"Invalid date format: '{date_str}'.")

    @staticmethod
    def _validate_string(value: Optional[str], field_name: str, max_len: int = 255, allow_empty: bool = True) -> str:
        """Validates and sanitizes string fields."""
        if value is None:
            return "UNKNOWN" if not allow_empty else ""

        value = str(value).strip()

        if not value:
            return "UNKNOWN" if not allow_empty else ""

        value = value.replace(APP_CONFIG["CSV_DELIMITER"], ',').replace('\n', ' ').replace('\r', '')

        return value[:max_len]

    @staticmethod
    def _validate_yes_no(value: Optional[str]) -> str:
        """Validates input for 'Yes'/'No' fields."""
        if value is None:
            return "NO"

        value = str(value).strip().lower()
        return "YES" if value in ('yes', 'y', 'true', '1') else "NO"

    @staticmethod
    def _validate_positive_int(value: Optional[str], field_name: str, min_val: int = 0) -> int:
        """Validates and converts a value to a non-negative integer."""
        try:
            if value is None or value == '':
                return min_val

            num = int(float(value))
            if num < min_val:
                logger.warning(f"Integer value for {field_name} ({num}) is less than minimum {min_val}. Setting to {min_val}.")
                return min_val
            return num
        except (ValueError, TypeError):
            logger.warning(f"Invalid integer value '{value}' for {field_name}. Setting to {min_val}.")
            return min_val

    @staticmethod
    def _validate_positive_float(value: Optional[str], field_name: str, min_val: float = 0.0) -> float:
        """Validates and converts a value to a non-negative float."""
        try:
            if value is None or value == '':
                return min_val

            num = float(value)
            if num < min_val:
                logger.warning(f"Float value for {field_name} ({num}) is less than minimum {min_val}. Setting to {min_val}.")
                return min_val
            return round(num, 2)
        except (ValueError, TypeError):
            logger.warning(f"Invalid float value '{value}' for {field_name}. Setting to {min_val}.")
            return min_val

    @staticmethod
    def _validate_range(value: Optional[str], field_name: str, min_val: int, max_val: int) -> int:
        """Validates a numeric field to be within a specific integer range."""
        try:
            if value is None or value == '':
                return min_val

            num = int(float(value))
            if not (min_val <= num <= max_val):
                logger.warning(f"Value for {field_name} ({num}) is out of range [{min_val}-{max_val}]. Setting to {min_val}.")
                return min_val
            return num
        except (ValueError, TypeError):
            logger.warning(f"Invalid numeric value '{value}' for {field_name}. Setting to {min_val}.")
            return min_val

    def validate(self) -> bool:
        """Performs comprehensive semantic validation on the DataObject's fields."""
        try:
            if not self.crash_date:
                raise DataValidationError("Crash date is required.")
            if not self.crash_type or self.crash_type == "UNKNOWN":
                raise DataValidationError("Crash type is required.")
            if not isinstance(self.num_units, int) or self.num_units < 0:
                raise DataValidationError("Number of units must be a non-negative integer.")
            if not isinstance(self.injuries_total, (int, float)) or self.injuries_total < 0:
                raise DataValidationError("Total injuries must be a non-negative number.")

            if self.crash_date:
                try:
                    datetime.strptime(self.crash_date, '%Y-%m-%d')
                except ValueError:
                    raise DataValidationError("Invalid crash date format (expected APAC-MM-DD).")

            total_reported_injuries = (
                self.injuries_fatal + self.injuries_incapacitating +
                self.injuries_non_incapacitating + self.injuries_reported_not_evident +
                self.injuries_no_indication
            )
            if self.injuries_total < total_reported_injuries - 0.01:
                logger.warning(f"Total injuries ({self.injuries_total}) less than sum of specific injuries ({total_reported_injuries}).")

            if not (0 <= self.crash_hour <= 23):
                raise DataValidationError("Crash hour out of valid range (0-23).")
            if not (1 <= self.crash_day_of_week <= 7):
                raise DataValidationError("Crash day of week out of valid range (1-7).")
            if not (1 <= self.crash_month <= 12):
                raise DataValidationError("Crash month out of valid range (1-12).")

            return True
        except DataValidationError as e:
            logger.warning(f"DataObject validation failed: {str(e)}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error during DataObject validation: {traceback.format_exc()}")
            return False

    def to_dict(self) -> Dict[str, Any]:
        """Converts the DataObject instance into a dictionary."""
        data = {field: getattr(self, field) for field in FIELDS}
        if self.id is not None: # Include ID if it exists
            data['id'] = self.id
        return data

    @classmethod
    def from_dict(cls, data_dict: Dict[str, Any]) -> 'DataObject':
        """Creates a DataObject instance from a dictionary."""
        return cls(existing_data_dict=data_dict)

    def __repr__(self) -> str:
        """Provides a string representation of the DataObject for debugging."""
        return f"DataObject(ID={self.id}, Date='{self.crash_date}', Type='{self.crash_type}', TotalInjuries={self.injuries_total})"

# --- Database Class (Adapted from v4epsilon - more robust backend) ---
# Data Structure: record_id (unsigned long long - 8 bytes), timestamp (double), data_hash (32 bytes for hex hash), is_valid (boolean)
RECORD_FORMAT = "<Q d 32s ?"
RECORD_HEADER_SIZE = struct.calcsize(RECORD_FORMAT)

class TrafficAccidentsDB:
    """
    Handles all database operations for traffic accident records.
    Implements file-based storage with robust locking, backups, indexing,
    and auto-increment ID management.
    """
    def __init__(self, db_file: str, index_file: str, lock_file: str, id_counter_file: str, backup_dir: str):
        self.db_file = db_file
        self.index_file = index_file
        self.lock_file = lock_file
        self.id_counter_file = id_counter_file
        self.backup_dir = backup_dir
        self._index_cache: Dict[int, int] = {}  # In-memory cache for the index (ID -> Position)
        self._current_id = 0  # Auto-increment ID counter

        self._ensure_directories()
        self._initialize_db_file()
        self._load_id_counter()
        self._load_index_to_cache()
        self._lock = filelock.FileLock(self.lock_file) # Initialize filelock here

    def _ensure_directories(self):
        """Ensures the database and index directories exist."""
        try:
            Path(self.db_file).parent.mkdir(parents=True, exist_ok=True)
            Path(self.index_file).parent.mkdir(parents=True, exist_ok=True)
            Path(self.id_counter_file).parent.mkdir(parents=True, exist_ok=True)
            Path(self.backup_dir).mkdir(parents=True, exist_ok=True) # Ensure backup dir exists
            logger.info("Ensured all necessary directories exist.")
        except OSError as e:
            logger.critical(f"Critical: Cannot create database directories. Error: {e}")
            raise DatabaseError(f"Failed to create necessary directories: {str(e)}")

    def _initialize_db_file(self):
        """Creates the database file if it doesn't exist."""
        if not Path(self.db_file).exists():
            try:
                with open(self.db_file, 'wb') as f:
                    pass  # Create an empty file
                logger.info(f"Database file created: {self.db_file}")
            except IOError as e:
                logger.critical(f"Critical: Cannot create database file {self.db_file}. Error: {e}")
                raise DatabaseError(f"Failed to create database file: {str(e)}")

    def _load_id_counter(self):
        """Loads the last used ID from the counter file."""
        if Path(self.id_counter_file).exists():
            try:
                with filelock.FileLock(self.lock_file, timeout=10):
                    with open(self.id_counter_file, 'r') as f:
                        self._current_id = int(f.read().strip())
                logger.info(f"ID counter loaded: {self._current_id}")
            except (IOError, ValueError) as e:
                logger.error(f"Error loading ID counter from {self.id_counter_file}: {e}. Resetting to 0.")
                self._current_id = 0
            except filelock.Timeout:
                logger.error(f"Could not acquire lock for ID counter file {self.lock_file} to load counter.")
                self._current_id = 0
        else:
            logger.info("ID counter file does not exist. Starting ID from 0.")
            self._current_id = 0
            self._save_id_counter()  # Create file with initial 0

    def _save_id_counter(self):
        """Saves the current ID counter to the file."""
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.id_counter_file, 'w') as f:
                    f.write(str(self._current_id))
            logger.info(f"ID counter saved: {self._current_id}")
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for ID counter file {self.lock_file} to save counter.")
            raise FileLockError(f"Failed to save ID counter: {self.lock_file} timed out.")
        except IOError as e:
            logger.error(f"Error saving ID counter to {self.id_counter_file}: {e}")
            raise DatabaseError(f"Failed to save ID counter: {str(e)}")

    def _get_next_id(self) -> int:
        """Increments and returns the next ID."""
        self._current_id += 1
        self._save_id_counter()
        return self._current_id

    def _calculate_data_hash(self, data: Dict[str, Any]) -> str:
        """Calculates SHA256 hash of the data content."""
        # Remove 'id' field from data before hashing to ensure hash is based purely on content
        # This prevents hash changes if only the ID is changed during update but content is same
        data_to_hash = {k: v for k, v in data.items() if k != 'id'}
        data_string = json.dumps(data_to_hash, sort_keys=True).encode('utf-8')
        return hashlib.sha256(data_string).hexdigest()

    def _load_index_to_cache(self):
        """Loads the index file into the in-memory cache."""
        self._index_cache = {}
        if not Path(self.index_file).exists():
            logger.info("Index file does not exist. Starting with an empty index cache.")
            return

        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.index_file, 'rb') as f_idx:
                    entry_size = struct.calcsize("<Q Q")
                    while True:
                        chunk = f_idx.read(entry_size)
                        if not chunk:
                            break
                        if len(chunk) == entry_size:
                            r_id, pos = struct.unpack("<Q Q", chunk)
                            self._index_cache[r_id] = pos
                        else:
                            logger.warning(f"Incomplete index entry found. Skipping remaining index file during load.")
                            break
            logger.info(f"Index loaded to cache. {len(self._index_cache)} entries.")
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for index file {self.lock_file} to load index cache.")
            self._index_cache = {}
            raise FileLockError(f"Failed to load index to cache: {self.lock_file} timed out.")
        except Exception as e:
            logger.error(f"Error loading index file {self.index_file} to cache: {e}")
            self._index_cache = {}
            raise DatabaseError(f"Failed to load index to cache: {str(e)}")

    def _save_index_from_cache(self):
        """Saves the in-memory index cache to the index file."""
        logger.info("Saving index cache to disk...")
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.index_file, 'wb') as f_idx:
                    for r_id, pos in self._index_cache.items():
                        f_idx.write(struct.pack("<Q Q", r_id, pos))
            logger.info(f"Index cache saved to disk. {len(self._index_cache)} entries.")
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for index file {self.lock_file} to save index cache.")
            raise FileLockError(f"Failed to save index from cache: {self.lock_file} timed out.")
        except Exception as e:
            logger.error(f"Error saving index cache to file {self.index_file}: {e}")
            raise DatabaseError(f"Failed to save index from cache: {str(e)}")

    def _update_index_cache_entry(self, record_id: int, position: int):
        """Updates a single entry in the in-memory index cache and saves it to disk."""
        self._index_cache[record_id] = position
        self._save_index_from_cache()

    def rebuild_index(self):
        """
        Rebuilds the index file from scratch, including only valid records.
        This updates the in-memory cache and then persists it to disk.
        """
        logger.info("Rebuilding index file from DB and updating cache...")
        new_index_data = {}
        max_id_in_db = 0
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'rb') as f_db:
                    f_db.seek(0, os.SEEK_END)
                    file_size = f_db.tell()
                    f_db.seek(0, os.SEEK_SET)

                    while f_db.tell() < file_size:
                        start_pos = f_db.tell()
                        header_bytes = f_db.read(RECORD_HEADER_SIZE)
                        if not header_bytes:
                            break

                        if len(header_bytes) < RECORD_HEADER_SIZE:
                            logger.warning(f"Incomplete header at position {start_pos}. Skipping remaining file during index rebuild.")
                            break

                        try:
                            record_id, timestamp_float, data_hash_bytes, is_valid = struct.unpack(RECORD_FORMAT, header_bytes)
                        except struct.error:
                            logger.error(f"Failed to unpack header at position {start_pos}. Corrupted header. Skipping.")
                            f_db.seek(start_pos + RECORD_HEADER_SIZE + 4 + 1024) # Skip header + size + some data to avoid infinite loop
                            continue

                        data_size_bytes = f_db.read(4)
                        if len(data_size_bytes) < 4:
                            logger.warning(f"Incomplete data size at position {f_db.tell()}. Skipping remaining file during index rebuild.")
                            break

                        try:
                            data_size = struct.unpack("<I", data_size_bytes)[0]
                        except struct.error:
                            logger.error(f"Failed to unpack data size at position {f_db.tell()}. Corrupted data size. Skipping.")
                            f_db.seek(f_db.tell() + 1024) # Skip some data
                            continue

                        f_db.seek(data_size, os.SEEK_CUR)

                        if is_valid:
                            new_index_data[record_id] = start_pos
                            if record_id > max_id_in_db:
                                max_id_in_db = record_id

                self._index_cache = new_index_data
                self._save_index_from_cache()

                if max_id_in_db >= self._current_id:
                    self._current_id = max_id_in_db + 1
                    self._save_id_counter()

            logger.info("Index file rebuilt and cache updated successfully.")
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to rebuild index.")
            raise FileLockError(f"Failed to rebuild index: {self.lock_file} timed out.")
        except Exception as e:
            logger.error(f"Error rebuilding index: {traceback.format_exc()}")
            raise DatabaseError(f"Failed to rebuild index: {str(e)}")

    def get_record_position_from_index(self, record_id: int) -> Optional[int]:
        """Retrieves the position of a record from the in-memory index cache."""
        return self._index_cache.get(record_id)

    def _insert_record(self, record_data: Dict[str, Any], record_id: Optional[int] = None, is_valid: bool = True) -> (int, int):
        """
        Internal method to insert a record into the .db file.
        If record_id is None, a new auto-increment ID is generated.
        Returns a tuple of (record_id, position_in_db).
        """
        if record_id is None:
            record_id = self._get_next_id()

        timestamp = datetime.now().timestamp()
        data_hash = self._calculate_data_hash(record_data) # Hashing only content, not ID
        data_bytes = json.dumps(record_data).encode('utf-8')
        data_size = len(data_bytes)

        record_header = struct.pack(
            RECORD_FORMAT,
            record_id,
            timestamp,
            bytes.fromhex(data_hash),
            is_valid
        )

        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'ab') as f:
                    current_position = f.tell()
                    f.write(record_header)
                    f.write(struct.pack("<I", data_size))
                    f.write(data_bytes)

            logger.info(f"Record {record_id} inserted at position {current_position}.")
            return record_id, current_position
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to insert record.")
            raise FileLockError(f"Failed to insert record: {self.lock_file} timed out.")
        except IOError as e:
            logger.error(f"IOError inserting record: {e}")
            raise DatabaseError(f"Failed to insert record: {str(e)}")
        except Exception as e:
            logger.error(f"Error inserting record: {traceback.format_exc()}")
            raise DatabaseError(f"Failed to insert record: {str(e)}")

    def insert_data(self, data_object: DataObject) -> int:
        """Public method to insert new accident data."""
        data_dict = data_object.to_dict()
        # The 'id' field should not be part of the data dict being stored in the raw record data,
        # as it's part of the record header. Remove it for the hash calculation and storage.
        if 'id' in data_dict:
            del data_dict['id']

        record_id, position = self._insert_record(data_dict, is_valid=True)
        # Update the data_object's ID after successful insertion
        data_object.id = record_id
        self._update_index_cache_entry(record_id, position)
        return record_id

    def _read_record_at_position(self, position: int) -> Optional[Dict[str, Any]]:
        """Reads a single record from a specific byte position."""
        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'rb') as f:
                    f.seek(position)
                    header_bytes = f.read(RECORD_HEADER_SIZE)
                    if not header_bytes or len(header_bytes) < RECORD_HEADER_SIZE:
                        logger.warning(f"Incomplete header at position {position}.")
                        return None
                    record_id, timestamp_float, data_hash_bytes, is_valid = struct.unpack(RECORD_FORMAT, header_bytes)

                    data_size_bytes = f.read(4)
                    if not data_size_bytes or len(data_size_bytes) < 4:
                        logger.warning(f"Incomplete data size at position {f.tell()}.")
                        return None
                    data_size = struct.unpack("<I", data_size_bytes)[0]

                    data_bytes = f.read(data_size)
                    if not data_bytes or len(data_bytes) < data_size:
                        logger.warning(f"Incomplete data at position {f.tell()}. Expected {data_size} bytes, got {len(data_bytes)}.")
                        return None

                    data = json.loads(data_bytes.decode('utf-8'))
                    timestamp = datetime.fromtimestamp(timestamp_float)
                    data_hash = data_hash_bytes.hex()
                    return {
                        "record_id": record_id,
                        "timestamp": timestamp,
                        "data_hash": data_hash,
                        "is_valid": is_valid,
                        "data": data, # This 'data' is the original dictionary content
                        "position": position
                    }
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to read record at position {position}.")
            return None
        except (IOError, struct.error, json.JSONDecodeError) as e:
            logger.error(f"Data corruption or read error at position {position}: {e}. Record might be unreadable.")
            return None
        except Exception as e:
            logger.error(f"Unexpected error reading record at position {position}: {traceback.format_exc()}")
            return None

    def get_record_by_id(self, record_id: int) -> Optional[DataObject]:
        """Retrieves a single record by its ID using the index."""
        position = self.get_record_position_from_index(record_id)
        if position is None:
            logger.info(f"Record with ID {record_id} not found in index.")
            return None
        record_info = self._read_record_at_position(position)
        if record_info and record_info["is_valid"]:
            # Combine record_id with data before passing to DataObject.from_dict
            full_data_dict = {"id": record_info["record_id"], **record_info["data"]}
            return DataObject.from_dict(full_data_dict)
        return None

    def get_all_records(self) -> Dict[int, DataObject]:
        """Retrieves all valid records from the database using the index."""
        all_records = {}
        # Iterate over a copy of keys to avoid modification issues if index is rebuilt mid-loop
        for record_id in list(self._index_cache.keys()):
            record_obj = self.get_record_by_id(record_id)
            if record_obj:
                all_records[record_id] = record_obj
        return all_records

    def update_record(self, record_id: int, new_data_object: DataObject) -> bool:
        """
        Updates an existing record by marking the old one as invalid
        and inserting a new one.
        """
        old_position = self.get_record_position_from_index(record_id)
        if old_position is None:
            logger.warning(f"Attempted to update non-existent record with ID: {record_id}")
            return False

        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                # Mark old record as invalid
                with open(self.db_file, 'r+b') as f:
                    f.seek(old_position)
                    header_bytes = f.read(RECORD_HEADER_SIZE)
                    if len(header_bytes) < RECORD_HEADER_SIZE:
                        logger.error(f"Incomplete header for record {record_id} at position {old_position} during update invalidation.")
                        return False # Cannot invalidate properly

                    # Unpack, set is_valid to False, and repack
                    r_id, timestamp_float, data_hash_bytes, is_valid_old = struct.unpack(RECORD_FORMAT, header_bytes)

                    # Seek back to write only the is_valid flag
                    f.seek(old_position + struct.calcsize("<Q d 32s"))
                    f.write(struct.pack("?", False))

                logger.info(f"Marked old record {record_id} at position {old_position} as invalid.")

                # Insert the new record with the same ID
                # Ensure the new_data_object's ID is set to the correct record_id for consistency
                new_data_object.id = record_id
                new_data_dict = new_data_object.to_dict()
                # Remove the 'id' from the dictionary that gets stored as data, as it's in the header
                if 'id' in new_data_dict:
                    del new_data_dict['id']

                new_record_id, new_position = self._insert_record(new_data_dict, record_id=record_id, is_valid=True)
                self._update_index_cache_entry(new_record_id, new_position) # Update index to point to new record

            return True
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to update record.")
            raise FileLockError(f"Failed to update record: {self.lock_file} timed out.")
        except Exception as e:
            logger.error(f"Error updating record {record_id}: {traceback.format_exc()}")
            raise DatabaseError(f"Failed to update record {record_id}: {str(e)}")

    def delete_record(self, record_id: int) -> bool:
        """
        Deletes a record by marking it as invalid in the database file
        and removing it from the in-memory index cache.
        """
        position = self.get_record_position_from_index(record_id)
        if position is None:
            logger.warning(f"Attempted to delete non-existent record with ID: {record_id}")
            return False

        try:
            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'r+b') as f:
                    f.seek(position)
                    header_bytes = f.read(RECORD_HEADER_SIZE)
                    if len(header_bytes) < RECORD_HEADER_SIZE:
                        logger.error(f"Incomplete header for record {record_id} at position {position} during deletion invalidation.")
                        return False

                    # Unpack, set is_valid to False, and repack
                    r_id, timestamp_float, data_hash_bytes, is_valid_old = struct.unpack(RECORD_FORMAT, header_bytes)
                    f.seek(position + struct.calcsize("<Q d 32s")) # Seek to the boolean flag
                    f.write(struct.pack("?", False)) # Write False to mark as invalid

                if record_id in self._index_cache:
                    del self._index_cache[record_id]
                    self._save_index_from_cache() # Persist the change to the index file
                logger.info(f"Record {record_id} marked as invalid and removed from index.")
            return True
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to delete record.")
            raise FileLockError(f"Failed to delete record: {self.lock_file} timed out.")
        except Exception as e:
            logger.error(f"Error deleting record {record_id}: {traceback.format_exc()}")
            raise DatabaseError(f"Failed to delete record {record_id}: {str(e)}")

    def _create_backup(self):
        """Creates a timestamped backup of the database file and manages backup rotation."""
        try:
            if not os.path.exists(self.db_file):
                logger.info("No database file found to backup.")
                return

            # Clean up old backups first
            backups = sorted(Path(self.backup_dir).glob("backup_*.db"))
            while len(backups) >= APP_CONFIG["MAX_BACKUPS"]:
                oldest = backups.pop(0)
                try:
                    os.unlink(oldest)
                    logger.info(f"Removed old backup: {oldest}")
                except OSError as e:
                    logger.warning(f"Could not remove old backup '{oldest}': {e}")

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = os.path.join(self.backup_dir, f"backup_{timestamp}.db")

            with filelock.FileLock(self.lock_file, timeout=10):
                with open(self.db_file, 'rb') as src:
                    with open(backup_path, 'wb') as dst:
                        while True:
                            chunk = src.read(APP_CONFIG["CHUNK_SIZE"])
                            if not chunk:
                                break
                            dst.write(chunk)
            logger.info(f"Created database backup: {backup_path}")
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for database file {self.lock_file} to create backup.")
            raise FileLockError(f"Failed to create backup: {self.lock_file} timed out.")
        except Exception as e:
            logger.error(f"Backup failed: {traceback.format_exc()}")
            raise DatabaseError(f"Failed to create database backup: {str(e)}")


# --- Streamlit UI Components (Unified and adapted) ---

def display_activity_log():
    """Displays the recent activity log entries."""
    st.subheader("📝 Activity Log")
    log_file_path = LOG_FILE_PATH
    MAX_LOG_ENTRIES_DISPLAY = APP_CONFIG["MAX_LOG_ENTRIES_DISPLAY"]

    if not Path(log_file_path).exists():
        st.info("ℹ️ Arquivo de log de atividades não encontrado.")
        return

    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            log_lines = f.readlines()

        display_entries = []
        # Read from end of file to get most recent entries
        for line in reversed(log_lines):
            try:
                # Example log format: '2023-10-27 15:30:00,123 - __main__ - INFO - Message'
                parts = line.strip().split(' - ', 3) # Split by ' - ' up to 3 times
                if len(parts) == 4:
                    timestamp_str_full = parts[0]
                    log_level = parts[2]
                    message = parts[3]
                    # Format timestamp for display (e.g., remove milliseconds if present)
                    timestamp_parts = timestamp_str_full.split(',')
                    timestamp_str = timestamp_parts[0] # Take only the date and time, ignore milliseconds

                    if "CRITICAL" in log_level.upper():
                        display_entries.append(f"**<span style='color:red;'>CRITICAL</span>** `{timestamp_str}` `{message}`")
                    elif "ERROR" in log_level.upper():
                        display_entries.append(f"**<span style='color:orange;'>ERROR</span>** `{timestamp_str}` `{message}`")
                    elif "WARNING" in log_level.upper():
                        display_entries.append(f"**<span style='color:gold;'>WARNING</span>** `{timestamp_str}` `{message}`")
                    elif "INFO" in log_level.upper():
                        display_entries.append(f"**<span style='color:cyan;'>INFO</span>** `{timestamp_str}` `{message}`")
                    elif "DEBUG" in log_level.upper():
                        display_entries.append(f"**<span style='color:grey;'>DEBUG</span>** `{timestamp_str}` `{message}`")
                    else:
                        display_entries.append(f"**`{timestamp_str}`** `{log_level}` `{message}`") # Default for unknown levels

                    if len(display_entries) >= MAX_LOG_ENTRIES_DISPLAY:
                        break
            except Exception as e:
                logger.warning(f"Failed to parse log line for registry: {line.strip()} - {e}")
                continue

        if display_entries:
            for entry in display_entries:
                st.markdown(entry, unsafe_allow_html=True)
        else:
            st.info("ℹ️ Nenhum registro recente de atividade relevante encontrado no log.")
    except Exception as e:
        st.error(f"⚠️ Não foi possível ler o log de atividades: {str(e)}")
        logger.error(f"Error reading activity log: {traceback.format_exc()}")


def record_form_ui(data_object: Optional[DataObject] = None, is_edit: bool = False):
    """
    Renders the form for creating or editing a DataObject.
    Pre-fills fields if a data_object is provided for editing.
    Returns a dictionary of form data on submit.
    """
    form_key = f"record_form_{'edit' if is_edit else 'add'}"
    if is_edit and data_object and data_object.id is not None:
        form_key += f"_{data_object.id}" # Make key unique for each record being edited

    with st.form(key=form_key):
        st.subheader(f"{'Editar' if is_edit else 'Registrar Novo'} Registro de Acidente")

        if is_edit and data_object and data_object.id is not None:
            st.markdown(f"**ID do Registro:** `{data_object.id}`")

        col1, col2 = st.columns(2)
        with col1:
            crash_date_val = datetime.strptime(data_object.crash_date, '%Y-%m-%d').date() if data_object and data_object.crash_date else datetime.now().date()
            crash_date = st.date_input("Data do Acidente", value=crash_date_val)

            traffic_control_device = st.text_input("Dispositivo de Controle de Tráfego", value=data_object.traffic_control_device if data_object else "")
            lighting_condition = st.text_input("Condição de Iluminação", value=data_object.lighting_condition if data_object else "")
            trafficway_type = st.text_input("Tipo de Via", value=data_object.trafficway_type if data_object else "")
            roadway_surface_cond = st.text_input("Condição da Superfície da Via", value=data_object.roadway_surface_cond if data_object else "")
            crash_type = st.text_input("Tipo de Colisão", value=data_object.crash_type if data_object else "", help="Campo obrigatório")
            damage = st.text_input("Dano", value=data_object.damage if data_object else "")
            num_units = st.number_input("Número de Unidades Envolvidas", min_value=0, value=data_object.num_units if data_object else 0, step=1)
            injuries_total = st.number_input("Total de Feridos", min_value=0.0, value=data_object.injuries_total if data_object else 0.0, step=0.1, format="%.2f")
            injuries_incapacitating = st.number_input("Ferimentos Incapacitantes", min_value=0.0, value=data_object.injuries_incapacitating if data_object else 0.0, step=0.1, format="%.2f")
            injuries_reported_not_evident = st.number_input("Ferimentos Reportados (Não Evidentes)", min_value=0.0, value=data_object.injuries_reported_not_evident if data_object else 0.0, step=0.1, format="%.2f")
            crash_day_of_week = st.slider("Dia da Semana da Colisão (1=Seg, 7=Dom)", min_value=1, max_value=7, value=data_object.crash_day_of_week if data_object else 1)

        with col2:
            weather_condition = st.text_input("Condição Climática", value=data_object.weather_condition if data_object else "")
            first_crash_type = st.text_input("Primeiro Tipo de Colisão", value=data_object.first_crash_type if data_object else "")
            alignment = st.text_input("Alinhamento", value=data_object.alignment if data_object else "")
            road_defect = st.text_input("Defeito na Via", value=data_object.road_defect if data_object else "")

            # Find current index for selectbox
            intersection_index = 0
            if data_object and data_object.intersection_related_i == "YES":
                intersection_index = 1
            intersection_related_i = st.selectbox("Relacionado a Cruzamento?", ["NO", "YES"], index=intersection_index)

            prim_contributory_cause = st.text_input("Principal Causa Contributiva", value=data_object.prim_contributory_cause if data_object else "")
            most_severe_injury = st.text_input("Lesão Mais Grave", value=data_object.most_severe_injury if data_object else "")
            injuries_fatal = st.number_input("Ferimentos Fatais", min_value=0.0, value=data_object.injuries_fatal if data_object else 0.0, step=0.1, format="%.2f")
            injuries_non_incapacitating = st.number_input("Ferimentos Não Incapacitantes", min_value=0.0, value=data_object.injuries_non_incapacitating if data_object else 0.0, step=0.1, format="%.2f")
            injuries_no_indication = st.number_input("Sem Indicação de Ferimentos", min_value=0.0, value=data_object.injuries_no_indication if data_object else 0.0, step=0.1, format="%.2f")
            crash_hour = st.slider("Hora da Colisão (0-23)", min_value=0, max_value=23, value=data_object.crash_hour if data_object else 0)
            crash_month = st.slider("Mês da Colisão (1-12)", min_value=1, max_value=12, value=data_object.crash_month if data_object else 1)

        submit_button_label = "Atualizar Registro" if is_edit else "Adicionar Registro"
        submitted = st.form_submit_button(submit_button_label)

        if submitted:
            form_data = {
                'crash_date': crash_date.strftime('%Y-%m-%d'),
                'traffic_control_device': traffic_control_device,
                'weather_condition': weather_condition,
                'lighting_condition': lighting_condition,
                'first_crash_type': first_crash_type,
                'trafficway_type': trafficway_type,
                'alignment': alignment,
                'roadway_surface_cond': roadway_surface_cond,
                'road_defect': road_defect,
                'crash_type': crash_type,
                'intersection_related_i': intersection_related_i,
                'damage': damage,
                'prim_contributory_cause': prim_contributory_cause,
                'num_units': num_units,
                'most_severe_injury': most_severe_injury,
                'injuries_total': injuries_total,
                'injuries_fatal': injuries_fatal,
                'injuries_incapacitating': injuries_incapacitating,
                'injuries_non_incapacitating': injuries_non_incapacitating,
                'injuries_reported_not_evident': injuries_reported_not_evident,
                'injuries_no_indication': injuries_no_indication,
                'crash_hour': crash_hour,
                'crash_day_of_week': crash_day_of_week,
                'crash_month': crash_month
            }
            if is_edit and data_object and data_object.id is not None:
                form_data['id'] = data_object.id # Preserve the ID if it's an edit operation
            return form_data
    return None

def setup_ui():
    """Sets up the Streamlit UI for CRUD operations."""
    st.set_page_config(layout="wide", page_title="Sistema de Gerenciamento de Acidentes de Trânsito")
    st.title("Sistema de Gerenciamento de Acidentes de Trânsito")

    # Initialize DB (singleton pattern for Streamlit)
    if 'db' not in st.session_state:
        try:
            st.session_state.db = TrafficAccidentsDB(
                db_file=DB_PATH,
                index_file=INDEX_PATH,
                lock_file=LOCK_PATH,
                id_counter_file=ID_COUNTER_PATH,
                backup_dir=BACKUP_PATH
            )
            st.session_state.db.rebuild_index() # Ensure index is consistent on startup
            logger.info("Database initialized successfully.")
        except Exception as e:
            st.error(f"Erro crítico ao inicializar o banco de dados: {e}")
            logger.critical(f"Critical DB initialization error: {traceback.format_exc()}")
            st.stop() # Stop the app if DB cannot be initialized

    db = st.session_state.db

    # Sidebar for navigation
    st.sidebar.header("Navegação")
    page = st.sidebar.radio("Ir para", ["Registrar Novo", "Visualizar/Editar/Excluir", "Importar CSV", "Backup/Restore", "Log de Atividades"])

    # Page: Register New Record
    if page == "Registrar Novo":
        st.header("Adicionar Novo Registro de Acidente")
        new_record_data = record_form_ui()
        if new_record_data:
            try:
                new_data_object = DataObject.from_dict(new_record_data)
                record_id = db.insert_data(new_data_object)
                db._create_backup() # Create backup after successful insertion
                st.success(f"Registro adicionado com sucesso! ID: {record_id}")
                logger.info(f"New record added with ID: {record_id}")
            except DataValidationError as e:
                st.error(f"Erro de validação dos dados: {e}")
                logger.warning(f"Data validation error on add: {e}")
            except Exception as e:
                st.error(f"Erro ao adicionar registro: {e}")
                logger.error(f"Error adding record: {traceback.format_exc()}")

    # Page: View/Edit/Delete Records
    elif page == "Visualizar/Editar/Excluir":
        st.header("Visualizar, Editar ou Excluir Registros")

        records_data = db.get_all_records()
        # Sort records by ID for consistent display
        records_list = sorted(records_data.items(), key=lambda item: item[0])

        if not records_list:
            st.info("Nenhum registro encontrado no banco de dados.")
            # Clear edit state if no records are found
            if 'edit_record_id' in st.session_state:
                del st.session_state.edit_record_id
            if 'edit_record_data' in st.session_state:
                del st.session_state.edit_record_data
            return

        # Pagination
        MAX_RECORDS_PER_PAGE = APP_CONFIG["MAX_RECORDS_PER_PAGE"]
        total_pages = (len(records_list) + MAX_RECORDS_PER_PAGE - 1) // MAX_RECORDS_PER_PAGE
        if 'current_page' not in st.session_state:
            st.session_state.current_page = 1

        # Ensure current_page doesn't exceed total_pages after deletions
        if st.session_state.current_page > total_pages and total_pages > 0:
            st.session_state.current_page = total_pages
        elif total_pages == 0:
             st.session_state.current_page = 1 # Reset to 1 if no pages

        col_page1, col_page2, col_page3 = st.columns([1, 1, 8])
        with col_page1:
            if st.button("Página Anterior", disabled=(st.session_state.current_page == 1), key="prev_page_btn"):
                st.session_state.current_page -= 1
                st.rerun()
        with col_page2:
            if st.button("Próxima Página", disabled=(st.session_state.current_page == total_pages), key="next_page_btn"):
                st.session_state.current_page += 1
                st.rerun()
        with col_page3:
            st.write(f"Página {st.session_state.current_page} de {total_pages if total_pages > 0 else 1}")

        start_idx = (st.session_state.current_page - 1) * MAX_RECORDS_PER_PAGE
        end_idx = start_idx + MAX_RECORDS_PER_PAGE
        displayed_records = records_list[start_idx:end_idx]

        st.subheader("Registros Atuais")
        if displayed_records:
            for record_id, record_obj in displayed_records:
                # Use st.expander for each record to make the UI cleaner
                with st.expander(f"Registro ID: {record_id} - Data: {record_obj.crash_date} - Tipo: {record_obj.crash_type}"):
                    col_display, col_actions = st.columns([3, 1])
                    with col_display:
                        # Display record data as JSON, including the ID
                        st.json(record_obj.to_dict())
                    with col_actions:
                        # Edit button
                        if st.button(f"Editar {record_id}", key=f"edit_{record_id}"):
                            st.session_state.edit_record_id = record_id
                            st.session_state.edit_record_data = record_obj # Pass DataObject instance
                            st.rerun()

                        # Delete button
                        if st.button(f"Excluir {record_id}", key=f"delete_{record_id}"):
                            if st.warning(f"Tem certeza que deseja excluir o registro ID {record_id}?"):
                                try:
                                    if db.delete_record(record_id):
                                        st.success(f"Registro {record_id} excluído com sucesso.")
                                        logger.info(f"Record {record_id} deleted.")
                                        # Clear edit state if the deleted record was being edited
                                        if 'edit_record_id' in st.session_state and st.session_state.edit_record_id == record_id:
                                            del st.session_state.edit_record_id
                                            del st.session_state.edit_record_data
                                        st.session_state.db.rebuild_index() # Rebuild index to remove deleted from view
                                        st.rerun() # Refresh list
                                    else:
                                        st.error(f"Falha ao excluir registro {record_id}.")
                                except Exception as e:
                                    st.error(f"Erro ao excluir registro {record_id}: {e}")
                                    logger.error(f"Error deleting record {record_id}: {traceback.format_exc()}")

        # Edit form display
        if 'edit_record_id' in st.session_state and st.session_state.edit_record_id is not None:
            st.subheader(f"Editar Registro ID: {st.session_state.edit_record_id}")

            # Ensure the DataObject for editing still exists in the DB
            record_to_edit = db.get_record_by_id(st.session_state.edit_record_id)
            if record_to_edit is None:
                st.error(f"Registro com ID {st.session_state.edit_record_id} não encontrado para edição. Ele pode ter sido excluído.")
                del st.session_state.edit_record_id
                del st.session_state.edit_record_data
                st.rerun()
            else:
                updated_data = record_form_ui(record_to_edit, is_edit=True)
                if updated_data:
                    try:
                        # The form_ui already includes 'id' in updated_data for edits
                        updated_data_object = DataObject.from_dict(updated_data)
                        if db.update_record(st.session_state.edit_record_id, updated_data_object):
                            db._create_backup() # Create backup after successful update
                            st.success(f"Registro {st.session_state.edit_record_id} atualizado com sucesso!")
                            logger.info(f"Record {st.session_state.edit_record_id} updated.")
                            del st.session_state.edit_record_id # Clear edit state
                            del st.session_state.edit_record_data
                            st.rerun() # Refresh list to show updated data
                        else:
                            st.error(f"Falha ao atualizar registro {st.session_state.edit_record_id}.")
                    except DataValidationError as e:
                        st.error(f"Erro de validação dos dados para atualização: {e}")
                        logger.warning(f"Data validation error on update: {e}")
                    except Exception as e:
                        st.error(f"Erro ao atualizar registro {st.session_state.edit_record_id}: {e}")
                        logger.error(f"Error updating record {st.session_state.edit_record_id}: {traceback.format_exc()}")

    # Page: Import CSV
    elif page == "Importar CSV":
        st.header("Importar Registros de Arquivo CSV")
        uploaded_file = st.file_uploader("Escolha um arquivo CSV", type=["csv"])

        if uploaded_file is not None:
            if uploaded_file.size > APP_CONFIG["MAX_FILE_SIZE_MB"] * 1024 * 1024:
                st.error(f"Arquivo muito grande. O tamanho máximo permitido é {APP_CONFIG['MAX_FILE_SIZE_MB']} MB.")
                return

            try:
                # Use tempfile to save the uploaded file and then process it
                with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    temp_path = tmp_file.name

                imported_count = 0
                skipped_count = 0
                error_messages = []

                with open(temp_path, 'r', encoding='utf-8') as f:
                    # Use csv.reader to handle different delimiters
                    reader = csv.reader(f, delimiter=APP_CONFIG["CSV_DELIMITER"])
                    header = next(reader) # Skip header row

                    # Basic check for number of fields
                    if len(header) != len(FIELDS):
                        st.warning(f"O número de colunas no CSV ({len(header)}) não corresponde ao esperado ({len(FIELDS)}). Pode haver erros na importação.")

                    total_lines_approx = sum(1 for line in open(temp_path, 'r', encoding='utf-8')) - 1 # Recalculate total lines excluding header
                    if total_lines_approx <= 0:
                        st.info("O arquivo CSV está vazio ou contém apenas o cabeçalho.")
                        os.unlink(temp_path)
                        return

                    progress_bar = st.progress(0)
                    line_count = 0

                    f.seek(0) # Reset file pointer after counting lines
                    next(f) # Skip header again for actual processing

                    for i, row in enumerate(reader):
                        line_count += 1
                        try:
                            # Create DataObject from row_data (no ID yet)
                            data_object = DataObject(row_data=row)
                            # insert_data will assign ID and update data_object.id
                            db.insert_data(data_object)
                            imported_count += 1
                        except (DataValidationError, ValueError) as e:
                            skipped_count += 1
                            error_messages.append(f"Linha {i+2}: Erro de validação/formato - {e} (Dados: {row})")
                            logger.warning(f"Skipping CSV row {i+2} due to error: {e} | Row: {row}")
                        except Exception as e:
                            skipped_count += 1
                            error_messages.append(f"Linha {i+2}: Erro inesperado - {e} (Dados: {row})")
                            logger.error(f"Unexpected error processing CSV row {i+2}: {traceback.format_exc()} | Row: {row}")

                        # Update progress bar
                        progress_bar.progress(min(100, int((line_count / total_lines_approx) * 100)))

                os.unlink(temp_path) # Clean up temp file

                db._create_backup() # Create backup after successful import
                db.rebuild_index() # Rebuild index after import to reflect all new records

                st.success(f"Importação concluída. Registros importados: {imported_count}. Registros ignorados: {skipped_count}.")
                if error_messages:
                    with st.expander("Mostrar Detalhes dos Erros de Importação"):
                        for msg in error_messages:
                            st.markdown(f"- {msg}")

            except Exception as e:
                st.error(f"Erro ao processar o arquivo CSV: {e}")
                logger.error(f"Error processing CSV file: {traceback.format_exc()}")

    # Page: Backup/Restore
    elif page == "Backup/Restore":
        st.header("Backup e Restauração do Banco de Dados")

        st.subheader("Criar Backup Manual")
        if st.button("Criar Backup Agora"):
            try:
                db._create_backup()
                st.success("Backup criado com sucesso!")
                logger.info("Manual backup created.")
            except Exception as e:
                st.error(f"Erro ao criar backup: {e}")
                logger.error(f"Error creating manual backup: {traceback.format_exc()}")

        st.subheader("Restaurações de Backup")
        backup_files = sorted(Path(BACKUP_PATH).glob("backup_*.db"), reverse=True)
        backup_options = {f.name: f for f in backup_files}

        if not backup_options:
            st.info("Nenhum arquivo de backup encontrado.")
            return

        selected_backup_name = st.selectbox("Selecione um arquivo de backup para restaurar:", list(backup_options.keys()))

        if st.button(f"Restaurar de {selected_backup_name}"):
            if st.warning(f"⚠️ Atenção: Isso substituirá o banco de dados atual com o backup selecionado. Tem certeza?"):
                selected_backup_path = backup_options[selected_backup_name]
                try:
                    # Acquire lock before manipulating DB files
                    with filelock.FileLock(LOCK_PATH, timeout=10):
                        # Make a temporary backup of the current DB before overwriting
                        temp_current_db_backup = DB_PATH + ".temp_backup"
                        if Path(DB_PATH).exists():
                            import shutil
                            shutil.copy2(DB_PATH, temp_current_db_backup)
                            logger.info(f"Temporarily backed up current DB to {temp_current_db_backup}")

                        # Copy the selected backup to the main DB file
                        shutil.copy2(selected_backup_path, DB_PATH)
                        # Also restore the index and id_counter from backup, or rebuild
                        # For simplicity, we'll rebuild index and reset ID counter based on new DB
                        if Path(INDEX_PATH).exists():
                            os.remove(INDEX_PATH)
                        if Path(ID_COUNTER_PATH).exists():
                            os.remove(ID_COUNTER_PATH)

                    st.session_state.db = TrafficAccidentsDB( # Re-initialize DB to reload state
                        db_file=DB_PATH,
                        index_file=INDEX_PATH,
                        lock_file=LOCK_PATH,
                        id_counter_file=ID_COUNTER_PATH,
                        backup_dir=BACKUP_PATH
                    )
                    st.session_state.db.rebuild_index() # Rebuild index for the restored DB

                    if Path(temp_current_db_backup).exists():
                        os.remove(temp_current_db_backup) # Clean up temporary backup
                        logger.info(f"Removed temporary backup {temp_current_db_backup}")

                    st.success(f"Banco de dados restaurado com sucesso de {selected_backup_name}.")
                    logger.info(f"Database restored from {selected_backup_name}.")
                    st.rerun() # Refresh UI

                except filelock.Timeout:
                    st.error(f"Não foi possível adquirir o bloqueio para restaurar o banco de dados. Tente novamente mais tarde.")
                    logger.error(f"File lock timeout during restore: {traceback.format_exc()}")
                except Exception as e:
                    st.error(f"Erro ao restaurar o banco de dados: {e}")
                    logger.error(f"Error restoring database from {selected_backup_name}: {traceback.format_exc()}")

    # Page: Activity Log
    elif page == "Log de Atividades":
        display_activity_log()

# --- Main Application Entry Point ---
if __name__ == "__main__":
    try:
        # Ensure base directories are created before any DB operations
        Path(APP_CONFIG["DB_DIR"]).mkdir(parents=True, exist_ok=True)
        Path(BACKUP_PATH).mkdir(parents=True, exist_ok=True)
    except OSError as e:
        st.error(f"Crítico: Não foi possível criar os diretórios do banco de dados. Verifique as permissões para {APP_CONFIG['DB_DIR']}. Erro: {e}")
        logger.critical(f"Initial directory creation failed: {traceback.format_exc()}")
        st.stop()

    setup_ui()

"""I cannot directly show you this code on Google Colab by creating a notebook and running it there for you. However, I can provide the complete Python code, which you can then paste into a Google Colab notebook.

Please note that this code is a Streamlit application, which is typically designed to run locally as a web application. Running a Streamlit application directly within a Google Colab environment usually requires additional setup (like installing Streamlit and potentially using a tool like `pyngrok` to expose the local server) to make the web interface accessible.

Here's the code again, which you can copy and paste into a Google Colab notebook:

<div class="md-recitation">
  Sources
  <ol>
  <li><a href="https://github.com/techmovie/notion-telegram-bot">https://github.com/techmovie/notion-telegram-bot</a></li>
  <li><a href="https://github.com/marcosvppfernandes/chicagoteam_quanggang">https://github.com/marcosvppfernandes/chicagoteam_quanggang</a></li>
  <li><a href="https://github.com/jmounika16/Power-BI_dashboards">https://github.com/jmounika16/Power-BI_dashboards</a></li>
  <li><a href="https://github.com/ajr930/Chicago_crash_data">https://github.com/ajr930/Chicago_crash_data</a></li>
  <li><a href="https://github.com/eric20132/Chicago-Crash-Damage-Prediction">https://github.com/eric20132/Chicago-Crash-Damage-Prediction</a></li>
  <li><a href="https://rstudio-pubs-static.s3.amazonaws.com/1178434_98a1fc90657348c3ba4c5cf80512054f.html">https://rstudio-pubs-static.s3.amazonaws.com/1178434_98a1fc90657348c3ba4c5cf80512054f.html</a></li>
  <li><a href="https://github.com/ndarwich/Crash-Analysis">https://github.com/ndarwich/Crash-Analysis</a></li>
  </ol>
</div>
"""