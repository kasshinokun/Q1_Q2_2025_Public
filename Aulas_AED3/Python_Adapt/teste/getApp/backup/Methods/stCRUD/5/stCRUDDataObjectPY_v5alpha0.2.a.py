# -*- coding: utf-8 -*-
"""Se possível mantenha TrafficAccidentsDB com os se...

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124ImEsXBpw9JWKdBLQqywBgkvRcEJUic

Com certeza! Entendi a sua necessidade de ter a implementação da árvore B em disco (agora chamada `TrafficAccidentsTree`) separada, mas sem que ela substitua o `TrafficAccidentsDB` atual na interface do Streamlit. Isso é ótimo para fins de comparação e para entender a arquitetura sem quebrar a funcionalidade existente.

Vou fazer as seguintes alterações:

1.  **Renomear `OptimizedBTree` para `TrafficAccidentsTree`**: Esta será a classe que encapsula a lógica da B-tree em disco.
2.  **Manter `TrafficAccidentsDB` como está**: Ele continuará usando seu sistema original de arquivo `.db` e `.idx` (embora o `.idx` provavelmente seja um pickle de dicionário).
3.  **Adicionar a `TrafficAccidentsTree` ao código**: Ela será definida, mas *não será instanciada* ou usada pelo `setup_ui()` principal.
4.  **Ajustar `DataObject`**: Os métodos `to_binary` e `from_binary` devem ser mais robustos para serializar/desserializar todos os campos. Vou expandir um pouco para campos comuns, mas você precisará ajustá-los para todos os campos do seu `DataObject` real para que a `TrafficAccidentsTree` funcione corretamente.

**Considerações importantes:**

* **Duplicação de Lógica CRUD:** A `TrafficAccidentsTree` terá seus próprios métodos `add_record`, `get_record`, `update_record`, `delete_record`, `get_all_records` (similar ao `TrafficAccidentsDB`). Isso é intencional para mostrar como a `TrafficAccidentsTree` agiria como um sistema de banco de dados independente.
* **Aviso de `NotImplementedError`:** As operações complexas da B-tree (deleção de nó interno, balanceamento, compactação de página) continuarão como `NotImplementedError` para manter o foco na arquitetura principal.
* **`APP_CONFIG` para `TrafficAccidentsTree`:** A `TrafficAccidentsTree` usará um arquivo de banco de dados diferente (`.btree_db`) para não conflitar com o `.db` do `TrafficAccidentsDB` atual.

Vamos ao código:
"""

# -*- coding: utf-8 -*-
import streamlit as st
import csv
import os
import struct
import json
import hashlib
import time
import filelock
import logging
from datetime import datetime, date
from pathlib import Path
from typing import List, Dict, Optional, Union, Callable, Any, Iterator
import tempfile
import traceback
import math
from collections import OrderedDict # Para cache LRU

# --- Configuration Constants (Centralized - Adapted from v4epsilon) ---
APP_CONFIG = {
    "DB_DIR": os.path.join(Path.home(), 'Documents', 'Data'),
    "DB_FILE_NAME": 'traffic_accidents.db',        # Usado por TrafficAccidentsDB (original)
    "INDEX_FILE_NAME": 'index.idx',                # Usado por TrafficAccidentsDB (original)
    "BTREE_DB_FILE_NAME": 'traffic_accidents_btree.db', # NOVO: Usado por TrafficAccidentsTree
    "LOCK_FILE_NAME": 'traffic_accidents.lock',
    "ID_COUNTER_FILE_NAME": 'id_counter.dat',
    "BACKUP_DIR_NAME": 'backups',
    "CSV_DELIMITER": ';',
    "MAX_RECORDS_PER_PAGE": 20,
    "MAX_FILE_SIZE_MB": 100,
    "CHUNK_SIZE": 4096, # Pode ser o PAGE_SIZE
    "MAX_BACKUPS": 5,
    "MAX_LOG_ENTRIES_DISPLAY": 10,
    "LOG_FILE_NAME": 'traffic_accidents.log'
}

# --- Setup Logging ---
log_dir = Path(APP_CONFIG["DB_DIR"])
log_dir.mkdir(parents=True, exist_ok=True)
log_file_path = log_dir / APP_CONFIG["LOG_FILE_NAME"]
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[
                        logging.FileHandler(log_file_path),
                        logging.StreamHandler()
                    ])

# --- DataObject Class (from your provided code - ADAPTED FOR BINARY SERIALIZATION) ---
class DataObject:
    def __init__(self, data: Dict[str, Any]):
        self.data = data
        self.id = data.get('id')

    def to_dict(self) -> Dict[str, Any]:
        return self.data

    @staticmethod
    def from_dict(data_dict: Dict[str, Any]):
        return DataObject(data_dict)

    # Adicionar métodos para comparar e serializar
    def __lt__(self, other):
        return self.id < other.id

    def __eq__(self, other):
        return self.id == other.id

    def __hash__(self):
        return hash(self.id)

    def to_binary(self) -> bytes:
        """
        Serializa o DataObject para bytes para armazenamento em disco.
        Adaptação das regras de tuple.rs.txt: Números big-endian, Strings com prefixo.
        ASSUMIR OS CAMPOS DO SEU stCRUDDataObject_v5alpha0.py para serialização.
        """
        serialized_data = b''

        # ID: String, para ser chave da B-tree. Usar 8 bytes fixos com padding para comparação rápida.
        # Ajuste o tamanho conforme o ID gerado pelo seu sistema.
        id_bytes = str(self.id).encode('utf-8').ljust(8, b'\x00')
        serialized_data += id_bytes

        # collision_type_code: String (ex: max 50 chars)
        collision_type_code_bytes = self.data.get('collision_type_code', '').encode('utf-8')
        # Use 1 byte para prefixo se max_len <= 255, 2 bytes se max_len <= 65535, etc.
        serialized_data += len(collision_type_code_bytes).to_bytes(1, 'little') + collision_type_code_bytes

        # trafficway_type: String (ex: max 100 chars)
        trafficway_type_bytes = self.data.get('trafficway_type', '').encode('utf-8')
        serialized_data += len(trafficway_type_bytes).to_bytes(1, 'little') + trafficway_type_bytes

        # street_name: String (ex: max 100 chars)
        street_name_bytes = self.data.get('street_name', '').encode('utf-8')
        serialized_data += len(street_name_bytes).to_bytes(1, 'little') + street_name_bytes

        # num_units: int (ex: 4 bytes big-endian)
        serialized_data += int(self.data.get('num_units', 0)).to_bytes(4, 'big', signed=True)

        # num_fatalities: int (ex: 4 bytes big-endian)
        serialized_data += int(self.data.get('num_fatalities', 0)).to_bytes(4, 'big', signed=True)

        # num_injuries: int (ex: 4 bytes big-endian)
        serialized_data += int(self.data.get('num_injuries', 0)).to_bytes(4, 'big', signed=True)

        # date_of_crash: String (ex: AAAA-MM-DD, 10 chars)
        date_of_crash_bytes = self.data.get('date_of_crash', '1970-01-01').encode('utf-8')
        serialized_data += len(date_of_crash_bytes).to_bytes(1, 'little') + date_of_crash_bytes

        # Adicione os outros campos aqui da mesma forma.
        # Para garantir a compatibilidade com a desserialização, a ORDEM IMPORTA.
        return serialized_data

    @staticmethod
    def from_binary(binary_data: bytes) -> 'DataObject':
        """
        Desserializa bytes de volta para DataObject.
        Deve CORRESPONDER EXATAMENTE à ordem e formato de to_binary.
        """
        data = {}
        offset = 0

        # ID
        id_bytes_read = binary_data[offset : offset + 8]
        data['id'] = id_bytes_read.decode('utf-8').strip('\x00')
        offset += 8

        # collision_type_code
        length = int.from_bytes(binary_data[offset:offset+1], 'little')
        offset += 1
        data['collision_type_code'] = binary_data[offset:offset+length].decode('utf-8')
        offset += length

        # trafficway_type
        length = int.from_bytes(binary_data[offset:offset+1], 'little')
        offset += 1
        data['trafficway_type'] = binary_data[offset:offset+length].decode('utf-8')
        offset += length

        # street_name
        length = int.from_bytes(binary_data[offset:offset+1], 'little')
        offset += 1
        data['street_name'] = binary_data[offset:offset+length].decode('utf-8')
        offset += length

        # num_units
        data['num_units'] = int.from_bytes(binary_data[offset:offset+4], 'big', signed=True)
        offset += 4

        # num_fatalities
        data['num_fatalities'] = int.from_bytes(binary_data[offset:offset+4], 'big', signed=True)
        offset += 4

        # num_injuries
        data['num_injuries'] = int.from_bytes(binary_data[offset:offset+4], 'big', signed=True)
        offset += 4

        # date_of_crash
        length = int.from_bytes(binary_data[offset:offset+1], 'little')
        offset += 1
        data['date_of_crash'] = binary_data[offset:offset+length].decode('utf-8')
        offset += length

        # Desserializar os outros campos aqui
        return DataObject(data)


# --- BTreeNodePage (Representação de Página em Memória para o Pager) ---
PAGE_SIZE = 4096 # Exemplo: 4KB por página
PAGE_HEADER_SIZE = 32 # Tamanho do cabeçalho da página (contém metadados)

class BTreeNodePage:
    """
    Representa o conteúdo binário de uma página de nó da B-tree,
    gerenciando cabeçalho, slots e espaço de dados.
    """
    def __init__(self, page_data: bytearray, page_num: int, is_new: bool = False):
        self.page_data = page_data
        self.page_num = page_num
        self.is_dirty = False

        if is_new:
            self._set_num_keys(0)
            self._set_is_leaf(True)
            self._set_free_space_offset(PAGE_SIZE) # Espaço livre do final para o início
            self._set_first_free_slot_idx(0) # Slots crescem do início para o fim
            self._set_next_leaf_page_num(0) # Iniciar com 0 (Nulo)
            self._set_parent_page_num(0) # Iniciar com 0 (Nulo)
            self.is_dirty = True

    # Métodos para acessar/modificar o cabeçalho
    def _get_header_value(self, offset: int, length: int) -> int:
        return int.from_bytes(self.page_data[offset : offset + length], 'little')
    def _set_header_value(self, offset: int, length: int, value: int):
        self.page_data[offset : offset + length] = value.to_bytes(length, 'little')
        self.is_dirty = True

    def get_num_keys(self) -> int: return self._get_header_value(0, 2) # 2 bytes
    def _set_num_keys(self, n: int): self._set_header_value(0, 2, n)
    def get_is_leaf(self) -> bool: return bool(self._get_header_value(2, 1)) # 1 byte
    def _set_is_leaf(self, is_leaf: bool): self._set_header_value(2, 1, int(is_leaf))
    def get_free_space_offset(self) -> int: return self._get_header_value(3, 2) # Offset do primeiro byte livre no final da página
    def _set_free_space_offset(self, offset: int): self._set_header_value(3, 2, offset)
    def get_first_free_slot_idx(self) -> int: return self._get_header_value(5, 2) # Próximo índice de slot livre
    def _set_first_free_slot_idx(self, idx: int): self._set_header_value(5, 2, idx)
    def get_next_leaf_page_num(self) -> int: return self._get_header_value(7, 4) # 4 bytes
    def _set_next_leaf_page_num(self, page_num: int): self._set_header_value(7, 4, page_num)
    def get_parent_page_num(self) -> int: return self._get_header_value(11, 4) # 4 bytes
    def _set_parent_page_num(self, page_num: int): self._set_header_value(11, 4, page_num)

    # --- Métodos para Slotted Page (Simplificado) ---
    SLOT_ENTRY_SIZE = 2 + 2 + 4 # offset (2 bytes) + length (2 bytes) + child_page_num (4 bytes)

    def _get_slot_entry(self, slot_idx: int) -> tuple[int, int, int]:
        offset = PAGE_HEADER_SIZE + slot_idx * SLOT_ENTRY_SIZE
        # Verificar se o acesso ao slot é dentro da área de slots usada
        if offset + SLOT_ENTRY_SIZE > self.get_free_space_offset() and not self.get_is_leaf():
            # Pode estar tentando ler um ponteiro para filho que está além das chaves
            # Isso é uma heurística para nós internos.
            if slot_idx == self.get_num_keys() and not self.get_is_leaf():
                return 0, 0, self._get_header_value(offset, 4) # Assume que o último ponteiro é direto
            raise IndexError(f"Slot {slot_idx} out of bounds for page {self.page_num}. Num keys: {self.get_num_keys()}")

        data_offset = self._get_header_value(offset, 2)
        data_length = self._get_header_value(offset + 2, 2)
        child_page_num = self._get_header_value(offset + 4, 4)
        return data_offset, data_length, child_page_num

    def _set_slot_entry(self, slot_idx: int, data_offset: int, data_length: int, child_page_num: int):
        offset = PAGE_HEADER_SIZE + slot_idx * SLOT_ENTRY_SIZE
        self._set_header_value(offset, 2, data_offset)
        self._set_header_value(offset + 2, 2, data_length)
        self._set_header_value(offset + 4, 4, child_page_num)
        self.is_dirty = True

    def get_free_space(self) -> int:
        return self.get_free_space_offset() - (PAGE_HEADER_SIZE + self.get_num_keys() * SLOT_ENTRY_SIZE)

    def insert_cell(self, slot_idx: int, key_payload: bytes, child_page_num: int = 0):
        num_keys = self.get_num_keys()
        payload_len = len(key_payload)
        required_space = SLOT_ENTRY_SIZE + payload_len

        if self.get_free_space() < required_space:
            raise ValueError(f"Page {self.page_num} is full. Available: {self.get_free_space()} bytes, Required: {required_space} bytes.")

        # Mover dados existentes para abrir espaço para o novo payload
        # Dados são armazenados do final da página para o início
        new_payload_offset = self.get_free_space_offset() - payload_len
        self.page_data[new_payload_offset : self.get_free_space_offset()] = key_payload
        self._set_free_space_offset(new_payload_offset)

        # Mover entradas de slot existentes para abrir espaço para o novo slot
        current_slots_end_byte = PAGE_HEADER_SIZE + num_keys * SLOT_ENTRY_SIZE
        # Copiar slots para a direita para abrir espaço
        self.page_data[PAGE_HEADER_SIZE + (slot_idx + 1) * SLOT_ENTRY_SIZE : current_slots_end_byte + SLOT_ENTRY_SIZE] = \
            self.page_data[PAGE_HEADER_SIZE + slot_idx * SLOT_ENTRY_SIZE : current_slots_end_byte]

        # Inserir a nova entrada de slot
        self._set_slot_entry(slot_idx, new_payload_offset, payload_len, child_page_num)
        self._set_num_keys(num_keys + 1)
        self.is_dirty = True

    def get_cell_data(self, slot_idx: int) -> bytes:
        data_offset, data_length, _ = self._get_slot_entry(slot_idx)
        return self.page_data[data_offset : data_offset + data_length]

    def get_child_page_num(self, slot_idx: int) -> int:
        _, _, child_page_num = self._get_slot_entry(slot_idx)
        return child_page_num

    def set_child_page_num(self, slot_idx: int, new_child_page_num: int):
        data_offset, data_length, _ = self._get_slot_entry(slot_idx)
        self._set_slot_entry(slot_idx, data_offset, data_length, new_child_page_num)

    def delete_cell(self, slot_idx: int):
        num_keys = self.get_num_keys()
        if slot_idx < 0 or slot_idx >= num_keys:
            raise IndexError("Slot index out of bounds")

        # Remover entrada do slot (movendo os slots restantes para a esquerda)
        for i in range(slot_idx, num_keys - 1):
            data_offset, data_length, child_page_num = self._get_slot_entry(i + 1)
            self._set_slot_entry(i, data_offset, data_length, child_page_num)

        # Opcional: zerar o último slot e o espaço de dados correspondente
        # Em uma implementação real, o espaço livre seria adicionado a uma lista
        # ou mapa de espaço livre na página para reutilização.
        last_slot_offset = PAGE_HEADER_SIZE + (num_keys - 1) * SLOT_ENTRY_SIZE
        self.page_data[last_slot_offset : last_slot_offset + SLOT_ENTRY_SIZE] = b'\x00' * SLOT_ENTRY_SIZE

        self._set_num_keys(num_keys - 1)
        self.is_dirty = True
        # A compactação do espaço de dados é um processo separado para reclamar espaço
        # e é muito mais complexa.

# --- Pager (Gerenciador de Páginas) ---
class Pager:
    def __init__(self, db_file_path: str, cache_size: int = 100):
        self.db_file_path = Path(db_file_path)
        self.db_file_path.parent.mkdir(parents=True, exist_ok=True) # Ensure dir exists

        self.file = open(self.db_file_path, 'r+b' if self.db_file_path.exists() else 'w+b')
        self.file.seek(0, os.SEEK_END)
        self.next_page_num = self.file.tell() // PAGE_SIZE

        # Gerenciamento da Página Zero (Metadados do DB) e Raiz da B-tree
        if self.next_page_num == 0: # Novo arquivo DB
            # Alocar Página Zero (metadados do DB)
            self._allocate_raw_page() # Página 0
            # Alocar Página Raiz da B-tree
            root_page = self.new_page(is_leaf=True)
            self.root_page_num = root_page.page_num # Root da B-tree é a página 1 inicialmente
            self._save_db_metadata() # Salva o número da página raiz na Página Zero
            logging.info(f"New B-Tree DB created at {self.db_file_path}. Root page: {self.root_page_num}")
        else: # Arquivo DB existente
            self._load_db_metadata() # Carrega o número da página raiz da Página Zero
            logging.info(f"Existing B-Tree DB loaded from {self.db_file_path}. Root page: {self.root_page_num}")

        self.cache = OrderedDict() # Cache LRU (PageNumber -> BTreeNodePage)
        self.cache_size = cache_size

    def __del__(self):
        # Garante que todas as páginas sujas sejam salvas ao fechar
        if hasattr(self, 'file') and not self.file.closed:
            self.flush_all()
            self.file.close()

    def _allocate_raw_page(self) -> int:
        """Aloca um novo bloco de disco de bytes brutos para uma página."""
        page_num = self.next_page_num
        self.file.seek(page_num * PAGE_SIZE)
        self.file.write(b'\x00' * PAGE_SIZE)
        self.next_page_num += 1
        return page_num

    def _save_db_metadata(self):
        # A página 0 contém metadados globais, como o número da página raiz da B-tree
        self.file.seek(0)
        # Formato: 4 bytes para root_page_num, 4 bytes para next_page_num, etc.
        self.file.write(self.root_page_num.to_bytes(4, 'little'))
        self.file.write(self.next_page_num.to_bytes(4, 'little')) # Salva o próximo num de página
        self.file.flush()

    def _load_db_metadata(self):
        self.file.seek(0)
        self.root_page_num = int.from_bytes(self.file.read(4), 'little')
        self.next_page_num = int.from_bytes(self.file.read(4), 'little') # Carrega o próximo num de página

    def read_page(self, page_num: int) -> BTreeNodePage:
        if page_num in self.cache:
            self.cache.move_to_end(page_num)
            return self.cache[page_num]

        if page_num * PAGE_SIZE >= self.file.tell():
            raise IndexError(f"Tentativa de ler página {page_num} fora dos limites do arquivo.")

        self.file.seek(page_num * PAGE_SIZE)
        page_data = bytearray(self.file.read(PAGE_SIZE))

        node_page = BTreeNodePage(page_data, page_num)

        if len(self.cache) >= self.cache_size:
            lru_page_num, lru_node_page = self.cache.popitem(last=False)
            if lru_node_page.is_dirty:
                self._write_page_to_disk(lru_node_page)

        self.cache[page_num] = node_page
        return node_page

    def write_page(self, node_page: BTreeNodePage):
        node_page.is_dirty = True
        if node_page.page_num not in self.cache:
            self.cache[node_page.page_num] = node_page
        self.cache.move_to_end(node_page.page_num)

    def flush_page(self, page_num: int):
        if page_num in self.cache:
            node_page = self.cache[page_num]
            if node_page.is_dirty:
                self._write_page_to_disk(node_page)
                node_page.is_dirty = False

    def flush_all(self):
        for page_num in list(self.cache.keys()):
            self.flush_page(page_num)
        self._save_db_metadata()

    def _write_page_to_disk(self, node_page: BTreeNodePage):
        self.file.seek(node_page.page_num * PAGE_SIZE)
        self.file.write(node_page.page_data)
        self.file.flush()

    def new_page(self, is_leaf: bool = True) -> BTreeNodePage:
        page_num = self._allocate_raw_page()
        new_page_data = bytearray(b'\x00' * PAGE_SIZE)
        node_page = BTreeNodePage(new_page_data, page_num, is_new=True)
        node_page._set_is_leaf(is_leaf)
        self.write_page(node_page)
        return node_page

    def free_page(self, page_num: int):
        if page_num in self.cache:
            del self.cache[page_num]
        # Em uma implementação real, esta página seria adicionada a uma lista de páginas livres.
        # Por simplicidade, para esta demo, apenas zera no disco.
        self.file.seek(page_num * PAGE_SIZE)
        self.file.write(b'\x00' * PAGE_SIZE)


# --- TrafficAccidentsTree (A nova classe B-tree para ser adicionada) ---
class TrafficAccidentsTree:
    """
    Uma implementação de B-tree em disco para gerenciar registros de acidentes.
    Esta classe encapsula a lógica da B-tree e interage com o Pager.
    """
    def __init__(self, db_file: str, order: int = 63):
        self.order = order
        self.t = math.ceil(order / 2) # Grau mínimo
        self.pager = Pager(db_file)
        self.root_page_num = self.pager.root_page_num
        logging.info(f"TrafficAccidentsTree initialized with B-Tree DB: {db_file}")

    def _get_node(self, page_num: int) -> BTreeNodePage:
        return self.pager.read_page(page_num)

    def _save_node(self, node_page: BTreeNodePage):
        self.pager.write_page(node_page)

    def _get_key_from_payload(self, payload_bytes: bytes) -> bytes:
        # Assumindo que a chave (ID) está nos primeiros 8 bytes do payload
        return payload_bytes[:8]

    def add_record(self, data_obj: DataObject):
        """
        Adiciona um novo registro na B-tree.
        """
        key_bytes = serialize_btree_key(data_obj.id)
        payload_bytes = data_obj.to_binary() # Payload completo do registro

        root_node_page = self._get_node(self.root_page_num)

        # Se a raiz está cheia, divide a raiz e aumenta a altura da árvore
        if root_node_page.get_num_keys() == (self.order - 1):
            new_root_page = self.pager.new_page(is_leaf=False)
            new_root_page.set_child_page_num(0, self.root_page_num) # Antiga raiz como primeiro filho

            # Atualizar o pai da antiga raiz
            root_node_page._set_parent_page_num(new_root_page.page_num)
            self._save_node(root_node_page)

            self.root_page_num = new_root_page.page_num
            self.pager.root_page_num = new_root_page.page_num # Atualizar Pager com nova raiz
            self._save_node(new_root_page)

            # Divide o filho (antiga raiz) e insere a nova chave
            self._split_child_disk(new_root_page.page_num, 0)
            self._insert_non_full_disk(new_root_page.page_num, key_bytes, payload_bytes)
        else:
            self._insert_non_full_disk(self.root_page_num, key_bytes, payload_bytes)
        self.pager.flush_all() # Gravar todas as alterações pendentes no disco
        logging.info(f"Record with ID {data_obj.id} added to TrafficAccidentsTree.")


    def _insert_non_full_disk(self, current_page_num: int, key_bytes: bytes, payload_bytes: bytes):
        node_page = self._get_node(current_page_num)
        num_keys = node_page.get_num_keys()
        is_leaf = node_page.get_is_leaf()

        i = num_keys - 1
        # Encontrar a posição correta para a chave
        while i >= 0:
            current_key_payload = node_page.get_cell_data(i)
            current_key_bytes_in_cell = self._get_key_from_payload(current_key_payload)

            if key_bytes == current_key_bytes_in_cell:
                # Chave já existe, atualizar o payload (comportamento de PUT)
                node_page.delete_cell(i) # Remove o antigo
                node_page.insert_cell(i, payload_bytes, node_page.get_child_page_num(i)) # Insere o novo
                self._save_node(node_page)
                logging.info(f"Key {deserialize_btree_key(key_bytes)} updated in page {current_page_num}")
                return
            elif key_bytes < current_key_bytes_in_cell:
                i -= 1
            else:
                break

        # O 'i' agora aponta para o índice onde a chave deve ser inserida (ou o filho para descer)
        i += 1

        if is_leaf:
            node_page.insert_cell(i, payload_bytes, 0) # Ponteiro de filho 0 para folhas
            self._save_node(node_page)
            logging.debug(f"Inserted key {deserialize_btree_key(key_bytes)} into leaf page {current_page_num} at slot {i}")
        else: # Nó interno
            child_page_num = node_page.get_child_page_num(i)
            child_node_page = self._get_node(child_page_num)

            if child_node_page.get_num_keys() == (self.order - 1): # Filho está cheio
                self._split_child_disk(current_page_num, i)

                # Re-ler o nó pai após o split, pois ele foi modificado
                node_page = self._get_node(current_page_num)

                # Re-avaliar qual filho ir depois que a chave mediana subiu
                # Agora o nó pai tem uma nova chave no índice `i`.
                # Precisamos verificar se a chave que queremos inserir é maior
                # que essa chave recém-promovida.
                promoted_key_payload = node_page.get_cell_data(i)
                promoted_key_bytes = self._get_key_from_payload(promoted_key_payload)
                if key_bytes > promoted_key_bytes:
                    i += 1 # Vai para o filho da direita (o novo nó)

            # Recursivamente insere no filho apropriado
            self._insert_non_full_disk(node_page.get_child_page_num(i), key_bytes, payload_bytes)


    def _split_child_disk(self, parent_page_num: int, child_index: int):
        parent_node_page = self._get_node(parent_page_num)
        child_node_page = self._get_node(parent_node_page.get_child_page_num(child_index))

        logging.debug(f"Splitting child page {child_node_page.page_num} for parent {parent_page_num}")

        new_child_node_page = self.pager.new_page(is_leaf=child_node_page.get_is_leaf())
        new_child_node_page._set_parent_page_num(parent_page_num)

        # Atualizar ponteiros de irmandade se for nó folha encadeado
        if child_node_page.get_is_leaf():
            new_child_node_page._set_next_leaf_page_num(child_node_page.get_next_leaf_page_num())
            child_node_page._set_next_leaf_page_num(new_child_node_page.page_num)

        # A chave mediana que será promovida para o pai
        median_key_payload = child_node_page.get_cell_data(self.t - 1)

        # Mover as chaves da direita do filho para o new_child_node_page
        # Começa da chave mediana + 1 (t-ésima chave) até o final
        keys_to_move = child_node_page.get_num_keys() - self.t
        for i in range(keys_to_move):
            key_payload = child_node_page.get_cell_data(self.t + i)
            child_ptr = 0
            if not child_node_page.get_is_leaf():
                child_ptr = child_node_page.get_child_page_num(self.t + i)
            new_child_node_page.insert_cell(i, key_payload, child_ptr)

        # Se não for folha, o último ponteiro de filho do nó original é o primeiro do novo nó
        if not child_node_page.get_is_leaf():
            new_child_node_page.set_child_page_num(keys_to_move, child_node_page.get_child_page_num(child_node_page.get_num_keys()))

        # Reduz o número de chaves no filho original
        child_node_page._set_num_keys(self.t - 1)

        # Insere a chave mediana no pai e adiciona o novo filho
        parent_node_page.insert_cell(child_index, median_key_payload, new_child_node_page.page_num)

        self._save_node(parent_node_page)
        self._save_node(child_node_page)
        self._save_node(new_child_node_page)
        logging.debug(f"Split completed. New child page: {new_child_node_page.page_num}")


    def get_record(self, record_id: str, current_page_num: int = None) -> Optional[Dict[str, Any]]:
        """
        Busca uma chave (ID do registro) e retorna o DataObject completo.
        """
        if current_page_num is None:
            current_page_num = self.root_page_num

        node_page = self._get_node(current_page_num)
        num_keys = node_page.get_num_keys()
        is_leaf = node_page.get_is_leaf()

        key_bytes_to_find = serialize_btree_key(record_id)

        i = 0
        while i < num_keys:
            current_key_payload = node_page.get_cell_data(i)
            current_key_bytes_in_cell = self._get_key_from_payload(current_key_payload)

            if key_bytes_to_find == current_key_bytes_in_cell:
                return DataObject.from_binary(current_key_payload).to_dict() # Chave encontrada
            elif key_bytes_to_find < current_key_bytes_in_cell:
                break
            i += 1

        if is_leaf:
            return None # Chave não encontrada em um nó folha
        else:
            child_page_num = node_page.get_child_page_num(i)
            return self.get_record(record_id, child_page_num)

    def update_record(self, record_id: str, new_data: Dict[str, Any]) -> bool:
        """
        Atualiza um DataObject existente na B-tree.
        Busca o registro, atualiza seus campos e reinsere.
        """
        existing_data_obj = self.get_record(record_id)
        if not existing_data_obj:
            logging.warning(f"Record {record_id} not found for update in TrafficAccidentsTree.")
            return False

        # Criar um DataObject atualizado (com o ID original)
        updated_data = existing_data_obj.copy()
        updated_data.update(new_data)
        updated_data['id'] = record_id # Garante que o ID não muda

        updated_data_obj = DataObject(updated_data)

        # Re-inserir o DataObject (a lógica de insert já trata a atualização se a chave existe)
        try:
            self.add_record(updated_data_obj) # Add record já faz o upsert (update se existe)
            logging.info(f"Record with ID {record_id} updated in TrafficAccidentsTree.")
            return True
        except Exception as e:
            logging.error(f"Failed to update record {record_id} in TrafficAccidentsTree: {e}")
            return False

    def delete_record(self, record_id: str) -> bool:
        """
        Deleta uma chave da B-tree.
        Esta é uma operação complexa em B-trees para manter as propriedades do nó.
        """
        key_bytes = serialize_btree_key(record_id)
        try:
            self._delete_from_node(self.root_page_num, key_bytes)
            self.pager.flush_all()
            logging.info(f"Record with ID {record_id} deleted from TrafficAccidentsTree.")
            return True
        except NotImplementedError:
            logging.warning(f"Delete operation for {record_id} not fully implemented in TrafficAccidentsTree.")
            return False
        except Exception as e:
            logging.error(f"Failed to delete record {record_id} from TrafficAccidentsTree: {e}")
            return False

    def _delete_from_node(self, current_page_num: int, key_bytes: bytes):
        node_page = self._get_node(current_page_num)
        num_keys = node_page.get_num_keys()
        is_leaf = node_page.get_is_leaf()

        # Encontrar a posição da chave
        i = 0
        found_key = False
        while i < num_keys:
            current_key_payload = node_page.get_cell_data(i)
            current_key_bytes_in_cell = self._get_key_from_payload(current_key_payload)
            if key_bytes == current_key_bytes_in_cell:
                found_key = True
                break
            elif key_bytes < current_key_bytes_in_cell:
                break
            i += 1

        if is_leaf:
            if found_key:
                node_page.delete_cell(i) # Remove a célula da folha
                self._save_node(node_page)
                logging.debug(f"Key {deserialize_btree_key(key_bytes)} deleted from leaf page {current_page_num} at slot {i}.")

                # Após a deleção, verificar se a folha está abaixo do mínimo (t-1 chaves)
                if current_page_num != self.root_page_num and node_page.get_num_keys() < (self.t - 1):
                    logging.warning(f"Leaf page {current_page_num} is underflow. Needs rebalancing.")
                    # Lógica para balancear (emprestar ou fundir com irmãos)
                    # Ex: self._balance_node(node_page.get_parent_page_num(), current_page_num)
                    raise NotImplementedError("Balanceamento após deleção em folha não implementado.")
            else:
                logging.warning(f"Key {deserialize_btree_key(key_bytes)} not found in leaf page {current_page_num} for deletion.")
        else: # Nó interno
            if found_key:
                # Caso 2: Chave está em nó interno
                # Muito complexo. Geralmente envolve:
                # a) Se filho 'i' tem pelo menos 't' chaves, encontrar predecessor na folha,
                #    substituir a chave do nó interno pelo predecessor e deletar o predecessor.
                # b) Se filho 'i' e 'i+1' ambos têm 't-1' chaves, fundi-los, mover a chave do pai para o nó fundido,
                #    e deletar recursivamente do nó fundido.
                raise NotImplementedError("Deleção de chave em nó interno não implementada totalmente.")
            else:
                child_page_num = node_page.get_child_page_num(i)
                child_node_page = self._get_node(child_page_num)

                # Se o filho tiver o mínimo de chaves, precisa balancear (emprestar ou fundir) ANTES de descer
                if child_node_page.get_num_keys() == (self.t - 1):
                    # Lógica de balanceamento: empréstimo de irmão ou fusão
                    # Muito complexo para este esqueleto.
                    raise NotImplementedError("Balanceamento para deleção em nó interno não implementado.")

                self._delete_from_node(child_page_num, key_bytes)

        # Se a raiz se tornar vazia e tiver apenas 1 filho (ou nenhum), o filho se torna a nova raiz
        # (Apenas se a raiz atual é interna e tem 0 chaves)
        root_node = self._get_node(self.root_page_num)
        if self.root_page_num == current_page_num and not root_node.get_is_leaf() and root_node.get_num_keys() == 0:
            if root_node.get_num_keys() == 0 and root_node.get_child_page_num(0) != 0: # Tem um filho
                self.root_page_num = root_node.get_child_page_num(0)
                self.pager.root_page_num = self.root_page_num
                self.pager.free_page(current_page_num) # Libera a antiga raiz
                logging.info(f"Root page changed to {self.root_page_num}")
            elif root_node.get_num_keys() == 0 and root_node.get_child_page_num(0) == 0: # Raiz vazia sem filhos (árvore vazia)
                logging.info("B-tree is now empty.")


    def get_all_records(self) -> List[Dict[str, Any]]:
        """
        Retorna todos os registros na árvore em ordem.
        """
        records = []
        for data_obj in self._traverse_in_order():
            records.append(data_obj.to_dict())
        logging.info(f"Retrieved {len(records)} records from TrafficAccidentsTree.")
        return records

    def _traverse_in_order(self, page_num: int = None) -> Iterator[DataObject]:
        """
        Percorre a árvore em ordem, retornando DataObjects.
        """
        if page_num is None:
            page_num = self.root_page_num

        node_page = self._get_node(page_num)
        num_keys = node_page.get_num_keys()
        is_leaf = node_page.get_is_leaf()

        for i in range(num_keys):
            if not is_leaf:
                child_page_num = node_page.get_child_page_num(i)
                yield from self._traverse_in_order(child_page_num)

            payload_bytes = node_page.get_cell_data(i)
            yield DataObject.from_binary(payload_bytes)

        if not is_leaf: # O último ponteiro de filho (depois da última chave)
            last_child_page_num = node_page.get_child_page_num(num_keys)
            yield from self._traverse_in_order(last_child_page_num)

    def compact_database(self):
        """
        A compactação de um DB B-tree é gerenciada pelo Pager e pela lógica de rebalanceamento/reutilização de páginas.
        """
        logging.warning("Compact database method for TrafficAccidentsTree is handled by internal B-tree logic.")
        self.pager.flush_all() # Garante que tudo seja escrito

# --- Original TrafficAccidentsDB (Mantido para compatibilidade com o frontend atual) ---
class TrafficAccidentsDB:
    def __init__(self, db_file_name: str):
        self.db_file = Path(APP_CONFIG["DB_DIR"]) / db_file_name
        self.idx_file = Path(APP_CONFIG["DB_DIR"]) / APP_CONFIG["INDEX_FILE_NAME"]
        self.id_counter_file = Path(APP_CONFIG["DB_DIR"]) / APP_CONFIG["ID_COUNTER_FILE_NAME"]
        self.lock_file = Path(APP_CONFIG["DB_DIR"]) / APP_CONFIG["LOCK_FILE_NAME"]
        self.db_dir = Path(APP_CONFIG["DB_DIR"])
        self.backup_dir = self.db_dir / APP_CONFIG["BACKUP_DIR_NAME"]

        self.db_dir.mkdir(parents=True, exist_ok=True)
        self.backup_dir.mkdir(exist_ok=True)

        self._db_handle = None
        self._next_id = 1
        self._load_next_id() # Ensure ID counter is loaded at init

        # Index stores: {id: offset}
        self.index: Dict[str, int] = self._load_index()

        logging.info(f"TrafficAccidentsDB initialized with flat file DB: {self.db_file}")

    def _open_db(self, mode: str):
        if self._db_handle is None or self._db_handle.closed:
            self._db_handle = open(self.db_file, mode)
        else:
            self._db_handle.seek(0, os.SEEK_END) # Ensure we're at the end for append

    def _close_db(self):
        if self._db_handle and not self._db_handle.closed:
            self._db_handle.close()
            self._db_handle = None

    def _load_index(self) -> Dict[str, int]:
        if self.idx_file.exists():
            try:
                with open(self.idx_file, 'r') as f:
                    return json.load(f)
            except json.JSONDecodeError:
                logging.warning("Index file corrupted or empty. Rebuilding index.")
                return self._rebuild_index()
        return {}

    def _save_index(self):
        with open(self.idx_file, 'w') as f:
            json.dump(self.index, f)

    def _load_next_id(self):
        if self.id_counter_file.exists():
            try:
                with open(self.id_counter_file, 'r') as f:
                    self._next_id = int(f.read().strip())
            except (ValueError, IOError) as e:
                logging.error(f"Error loading ID counter: {e}. Resetting to 1.")
                self._next_id = 1
        else:
            self._next_id = 1
        logging.info(f"Next ID loaded: {self._next_id}")

    def _save_next_id(self):
        try:
            with open(self.id_counter_file, 'w') as f:
                f.write(str(self._next_id))
        except IOError as e:
            logging.error(f"Error saving ID counter: {e}")

    def _get_next_id(self) -> str:
        new_id = str(self._next_id).zfill(8)  # Pad with zeros to 8 digits
        self._next_id += 1
        self._save_next_id()
        return new_id

    def _read_record_at_offset(self, offset: int) -> Optional[Dict[str, Any]]:
        self._open_db('r+b')
        self._db_handle.seek(offset)

        # Read length prefix (4 bytes for length)
        length_bytes = self._db_handle.read(4)
        if not length_bytes:
            return None # EOF or corrupt

        record_length = struct.unpack('<I', length_bytes)[0]

        record_bytes = self._db_handle.read(record_length)
        if len(record_bytes) != record_length:
            logging.warning(f"Incomplete record read at offset {offset}. Expected {record_length} bytes, got {len(record_bytes)}")
            return None

        # Simple JSON serialization (original implementation)
        try:
            data_str = record_bytes.decode('utf-8')
            return json.loads(data_str)
        except (json.JSONDecodeError, UnicodeDecodeError) as e:
            logging.error(f"Error decoding record at offset {offset}: {e}")
            return None

    def _write_record(self, record_data: Dict[str, Any], offset: Optional[int] = None) -> int:
        data_str = json.dumps(record_data, ensure_ascii=False)
        record_bytes = data_str.encode('utf-8')
        record_length = len(record_bytes)

        # Prefix with length (4 bytes, unsigned int, little-endian)
        prefixed_record = struct.pack('<I', record_length) + record_bytes

        self._open_db('r+b')
        if offset is None: # Append to end of file
            self._db_handle.seek(0, os.SEEK_END)
            current_offset = self._db_handle.tell()
        else: # Overwrite at specific offset (used for updates)
            current_offset = offset
            self._db_handle.seek(offset)

        self._db_handle.write(prefixed_record)
        self._db_handle.flush()
        return current_offset

    def _rebuild_index(self) -> Dict[str, int]:
        logging.info("Rebuilding index...")
        rebuilt_index = {}
        self._open_db('r+b')
        self._db_handle.seek(0)

        current_offset = 0
        while True:
            # Read length prefix
            length_bytes = self._db_handle.read(4)
            if not length_bytes:
                break # EOF

            record_length = struct.unpack('<I', length_bytes)[0]

            # Read record data
            record_bytes = self._db_handle.read(record_length)
            if len(record_bytes) != record_length:
                logging.warning(f"Incomplete record found at offset {current_offset} during index rebuild. Skipping.")
                break # Corrupt or incomplete record

            try:
                data_str = record_bytes.decode('utf-8')
                record_data = json.loads(data_str)
                record_id = record_data.get('id')
                if record_id and record_data.get('status', 'active') == 'active': # Only index active records
                    rebuilt_index[record_id] = current_offset
                    # Update _next_id if necessary
                    if record_id.isdigit():
                        self._next_id = max(self._next_id, int(record_id) + 1)
            except (json.JSONDecodeError, UnicodeDecodeError) as e:
                logging.error(f"Error rebuilding index at offset {current_offset}: {e}. Skipping record.")

            current_offset += 4 + record_length # Move to next record

        self._save_next_id() # Save updated _next_id
        self.index = rebuilt_index
        self._save_index()
        self._close_db()
        logging.info(f"Index rebuilt. Total active records: {len(self.index)}")
        return self.index


    def add_record(self, record_data: Dict[str, Any]) -> str:
        with filelock.FileLock(self.lock_file, timeout=10):
            if 'id' not in record_data or not record_data['id']:
                record_data['id'] = self._get_next_id()
                logging.info(f"Generated ID: {record_data['id']}")

            record_id = record_data['id']
            record_data['status'] = 'active' # Mark as active

            offset = self._write_record(record_data)
            self.index[record_id] = offset
            self._save_index()
            logging.info(f"Record with ID {record_id} added successfully to flat file DB.")
            return record_id

    def get_record(self, record_id: str) -> Optional[Dict[str, Any]]:
        with filelock.FileLock(self.lock_file, timeout=10):
            offset = self.index.get(record_id)
            if offset is None:
                logging.info(f"Record with ID {record_id} not found in flat file DB index.")
                return None

            record = self._read_record_at_offset(offset)
            if record and record.get('status') == 'active':
                logging.info(f"Record with ID {record_id} retrieved successfully from flat file DB.")
                return record
            else:
                logging.warning(f"Record {record_id} found in index but not active/valid in flat file DB.")
                return None

    def update_record(self, record_id: str, new_data: Dict[str, Any]) -> bool:
        with filelock.FileLock(self.lock_file, timeout=10):
            existing_record = self.get_record(record_id)
            if not existing_record:
                logging.warning(f"Record with ID {record_id} not found for update in flat file DB.")
                return False

            # Update data, keeping original ID and marking old as 'deleted' logically
            existing_record['status'] = 'deleted' # Mark old record as deleted in DB file
            self._write_record(existing_record, self.index[record_id]) # Overwrite old record's status

            updated_data = existing_record.copy()
            updated_data.update(new_data)
            updated_data['id'] = record_id # Ensure ID doesn't change
            updated_data['status'] = 'active' # Mark new record as active

            # Write the new version of the record at the end of the file
            offset = self._write_record(updated_data)
            self.index[record_id] = offset # Update index to point to new record
            self._save_index()
            logging.info(f"Record with ID {record_id} updated successfully in flat file DB.")
            return True

    def delete_record(self, record_id: str) -> bool:
        with filelock.FileLock(self.lock_file, timeout=10):
            offset = self.index.get(record_id)
            if offset is None:
                logging.warning(f"Record with ID {record_id} not found for deletion in flat file DB.")
                return False

            # Logical deletion: Mark record as 'deleted' in DB file
            record_to_delete = self._read_record_at_offset(offset)
            if record_to_delete:
                record_to_delete['status'] = 'deleted'
                self._write_record(record_to_delete, offset) # Overwrite existing record
                del self.index[record_id] # Remove from index
                self._save_index()
                logging.info(f"Record with ID {record_id} logically deleted from flat file DB.")
                return True
            logging.warning(f"Could not retrieve record {record_id} at offset {offset} for deletion.")
            return False

    def get_all_records(self) -> List[Dict[str, Any]]:
        with filelock.FileLock(self.lock_file, timeout=10):
            records = []
            for record_id in list(self.index.keys()): # Iterate over a copy of keys
                record = self.get_record(record_id)
                if record:
                    records.append(record)
            logging.info(f"Retrieved {len(records)} active records from flat file DB.")
            return records

    def _create_backup(self, backup_file_name: str):
        # Create a timestamped backup
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        backup_path = self.backup_dir / f"{timestamp}_{backup_file_name}"

        # Copy the main DB file and index file
        import shutil
        if self.db_file.exists():
            shutil.copy2(self.db_file, backup_path)
            logging.info(f"Backup created: {backup_path}")
        if self.idx_file.exists():
            shutil.copy2(self.idx_file, self.backup_dir / f"{timestamp}_{APP_CONFIG['INDEX_FILE_NAME']}")
            logging.info(f"Index backup created: {self.backup_dir / f'{timestamp}_{APP_CONFIG['INDEX_FILE_NAME']}'}")

    def create_backup_advanced(self, public_key_rsa: Optional[Any] = None):
        """
        Creates an encrypted backup of the database.
        This would require cryptographic libraries and specific implementation.
        """
        logging.info("Attempting to create advanced backup...")
        # Placeholder for actual encryption logic
        backup_name = f"traffic_accidents_encrypted_backup_{datetime.now().strftime('%Y%m%d%H%M%S')}.enc"
        # Dummy operation for demonstration
        if self.db_file.exists():
            with open(self.db_file, 'rb') as f_in:
                # In a real scenario, encrypt f_in and write to backup_name
                # For demo, just copy (no encryption)
                with open(self.backup_dir / backup_name, 'wb') as f_out:
                    f_out.write(f_in.read())
            logging.info(f"Dummy encrypted backup created: {self.backup_dir / backup_name}")
        else:
            logging.warning("Database file not found for backup.")

        # Clean up old backups
        self._cleanup_old_backups()
        logging.info("Advanced backup process finished.")

    def restore_backup_advanced(self, backup_file_path: Path, private_key_rsa: Optional[Any] = None) -> bool:
        """
        Restores the database from an encrypted backup.
        This would require cryptographic libraries and specific implementation.
        """
        logging.info(f"Attempting to restore from advanced backup: {backup_file_path}")
        if not backup_file_path.exists():
            logging.error(f"Backup file not found: {backup_file_path}")
            return False

        try:
            # First, close current DB handles
            self._close_db()

            # Dummy restore (decryption and copy)
            with open(backup_file_path, 'rb') as f_in:
                with open(self.db_file, 'wb') as f_out:
                    f_out.write(f_in.read()) # In real scenario, decrypt f_in

            # Restore index file too if it was backed up
            idx_backup_path = backup_file_path.parent / f"{backup_file_path.stem.split('_')[0]}_{APP_CONFIG['INDEX_FILE_NAME']}"
            if idx_backup_path.exists():
                shutil.copy2(idx_backup_path, self.idx_file)

            # Rebuild index and reload ID counter after restore
            self.index = self._rebuild_index()
            self._load_next_id()
            logging.info(f"Database restored from {backup_file_path} and index rebuilt.")
            return True
        except Exception as e:
            logging.error(f"Failed to restore backup: {e}\n{traceback.format_exc()}")
            return False

    def compact_database(self):
        """
        Compacta o arquivo de banco de dados, removendo registros marcados como 'deleted'.
        Cria um novo arquivo e copia apenas os registros ativos.
        """
        with filelock.FileLock(self.lock_file, timeout=10):
            logging.info("Starting database compaction...")
            temp_db_file = self.db_file.with_suffix(".tmp")
            temp_idx_file = self.idx_file.with_suffix(".tmp")

            new_index = {}
            current_offset = 0

            self._open_db('r+b')
            self._db_handle.seek(0)

            with open(temp_db_file, 'wb') as new_db_f:
                while True:
                    # Read length prefix
                    length_bytes = self._db_handle.read(4)
                    if not length_bytes:
                        break # EOF

                    record_length = struct.unpack('<I', length_bytes)[0]
                    record_bytes = self._db_handle.read(record_length)

                    if len(record_bytes) != record_length:
                        logging.warning(f"Incomplete record found during compaction. Skipping remaining file.")
                        break # Corrupt or incomplete record

                    try:
                        record_data = json.loads(record_bytes.decode('utf-8'))
                        record_id = record_data.get('id')
                        status = record_data.get('status', 'active')

                        if status == 'active' and record_id:
                            # Write active record to new file
                            prefixed_record = struct.pack('<I', record_length) + record_bytes
                            new_db_f.write(prefixed_record)
                            new_index[record_id] = current_offset
                            current_offset += 4 + record_length
                        else:
                            logging.debug(f"Skipping deleted/invalid record with ID: {record_id}")
                    except (json.JSONDecodeError, UnicodeDecodeError) as e:
                        logging.error(f"Error decoding record during compaction: {e}. Skipping record.")

            self._close_db() # Close original DB handle

            # Replace original files with compacted ones
            os.replace(temp_db_file, self.db_file)
            with open(temp_idx_file, 'w') as f:
                json.dump(new_index, f)
            os.replace(temp_idx_file, self.idx_file)

            # Reload index and next_id
            self.index = new_index
            self._rebuild_index() # To ensure _next_id is correctly updated if IDs are numerical

            logging.info("Database compaction finished successfully.")

    def _cleanup_old_backups(self):
        """Removes oldest backups if they exceed MAX_BACKUPS."""
        all_backups = sorted(list(self.backup_dir.glob("traffic_accidents_encrypted_backup_*.enc"))) # Adjust pattern
        if len(all_backups) > APP_CONFIG["MAX_BACKUPS"]:
            for i in range(len(all_backups) - APP_CONFIG["MAX_BACKUPS"]):
                os.remove(all_backups[i])
                logging.info(f"Removed old backup: {all_backups[i]}")

# --- Streamlit UI (main application - adapted from your stCRUDDataObject_v5alpha0.py) ---
def setup_ui():
    st.set_page_config(page_title="Gerenciamento de Acidentes de Trânsito", layout="wide")
    st.title("Sistema de Gerenciamento de Acidentes de Trânsito")

    # Initialize DB (original flat file implementation)
    if 'db' not in st.session_state:
        db_file = APP_CONFIG["DB_FILE_NAME"]
        st.session_state.db = TrafficAccidentsDB(db_file)
        logging.info("TrafficAccidentsDB (original flat file) initialized.")

    # A TrafficAccidentsTree (B-Tree) será criada AQUI, mas NÃO USADA pelo Streamlit principal
    # para não conflitar com a implementação TrafficAccidentsDB.
    if 'btree_db' not in st.session_state:
        btree_db_file = APP_CONFIG["BTREE_DB_FILE_NAME"]
        st.session_state.btree_db = TrafficAccidentsTree(btree_db_file, order=63)
        logging.info("TrafficAccidentsTree (B-Tree in disk) initialized but NOT in use by UI.")
        # Exemplo de como você poderia testar a B-Tree aqui (sem exibir na UI):
        # try:
        #    test_obj = DataObject({"id": "00000001", "street_name": "Test St"})
        #    st.session_state.btree_db.add_record(test_obj)
        #    found_obj = st.session_state.btree_db.get_record("00000001")
        #    if found_obj:
        #        logging.info(f"B-Tree test: Found {found_obj['street_name']}")
        # except Exception as e:
        #    logging.error(f"B-Tree test failed: {e}")


    menu_options = ["Adicionar Registro", "Listar Registros", "Buscar Registro", "Atualizar Registro", "Deletar Registro", "Configurações"]
    selected_option = st.sidebar.selectbox("Menu", menu_options)

    if selected_option == "Adicionar Registro":
        st.header("Adicionar Novo Registro de Acidente")
        with st.form("add_record_form"):
            accident_type_code = st.text_input("Tipo de Acidente (código)", max_chars=50)
            collision_type_code = st.text_input("Tipo de Colisão (código)", max_chars=50)
            trafficway_type = st.text_input("Tipo de Via", max_chars=100)
            street_name = st.text_input("Nome da Rua", max_chars=100)
            num_units = st.number_input("Número de Unidades Envolvidas", min_value=1, step=1)
            num_fatalities = st.number_input("Número de Fatalidades", min_value=0, step=1)
            num_injuries = st.number_input("Número de Feridos", min_value=0, step=1)
            date_of_crash_str = st.text_input("Data do Acidente (AAAA-MM-DD)", value=date.today().strftime("%Y-%m-%d"))

            submitted = st.form_submit_button("Adicionar Registro")
            if submitted:
                new_record_data = {
                    "accident_type_code": accident_type_code,
                    "collision_type_code": collision_type_code,
                    "trafficway_type": trafficway_type,
                    "street_name": street_name,
                    "num_units": num_units,
                    "num_fatalities": num_fatalities,
                    "num_injuries": num_injuries,
                    "date_of_crash": date_of_crash_str,
                }
                try:
                    record_id = st.session_state.db.add_record(new_record_data) # Using original DB
                    st.success(f"Registro adicionado com sucesso! ID: {record_id}")
                except Exception as e:
                    st.error(f"Erro ao adicionar registro: {e}")
                    logging.error(f"Error adding record: {e}", exc_info=True)


    elif selected_option == "Listar Registros":
        st.header("Lista de Todos os Registros")
        records = st.session_state.db.get_all_records() # Using original DB
        if records:
            st.dataframe(records)
        else:
            st.info("Nenhum registro encontrado.")

    elif selected_option == "Buscar Registro":
        st.header("Buscar Registro por ID")
        record_id = st.text_input("ID do Registro a Buscar")
        if st.button("Buscar"):
            if record_id:
                record = st.session_state.db.get_record(record_id) # Using original DB
                if record:
                    st.json(record)
                else:
                    st.warning("Registro não encontrado.")
            else:
                st.error("Por favor, digite um ID para buscar.")

    elif selected_option == "Atualizar Registro":
        st.header("Atualizar Registro Existente")
        record_id_to_update = st.text_input("ID do Registro a Atualizar")

        if record_id_to_update:
            existing_record = st.session_state.db.get_record(record_id_to_update) # Using original DB
            if existing_record:
                st.write("Dados Atuais:")
                st.json(existing_record)

                with st.form("update_record_form"):
                    st.write("Insira os novos dados (deixe em branco para manter o atual):")
                    new_accident_type_code = st.text_input("Tipo de Acidente (código)", value=existing_record.get('accident_type_code', ''))
                    new_collision_type_code = st.text_input("Tipo de Colisão (código)", value=existing_record.get('collision_type_code', ''))
                    new_trafficway_type = st.text_input("Tipo de Via", value=existing_record.get('trafficway_type', ''))
                    new_street_name = st.text_input("Nome da Rua", value=existing_record.get('street_name', ''))
                    new_num_units = st.number_input("Número de Unidades Envolvidas", min_value=1, step=1, value=existing_record.get('num_units', 1))
                    new_num_fatalities = st.number_input("Número de Fatalidades", min_value=0, step=1, value=existing_record.get('num_fatalities', 0))
                    new_num_injuries = st.number_input("Número de Feridos", min_value=0, step=1, value=existing_record.get('num_injuries', 0))
                    new_date_of_crash_str = st.text_input("Data do Acidente (AAAA-MM-DD)", value=existing_record.get('date_of_crash', date.today().strftime("%Y-%m-%d")))

                    submitted = st.form_submit_button("Atualizar Registro")
                    if submitted:
                        updated_data = {
                            "accident_type_code": new_accident_type_code,
                            "collision_type_code": new_collision_type_code,
                            "trafficway_type": new_trafficway_type,
                            "street_name": new_street_name,
                            "num_units": new_num_units,
                            "num_fatalities": new_num_fatalities,
                            "num_injuries": new_num_injuries,
                            "date_of_crash": new_date_of_crash_str,
                        }
                        try:
                            if st.session_state.db.update_record(record_id_to_update, updated_data): # Using original DB
                                st.success(f"Registro {record_id_to_update} atualizado com sucesso!")
                                st.experimental_rerun()
                            else:
                                st.error("Falha ao atualizar o registro.")
                        except Exception as e:
                            st.error(f"Erro ao atualizar registro: {e}")
                            logging.error(f"Error updating record: {e}", exc_info=True)
            else:
                st.warning("Registro não encontrado para o ID fornecido.")
        else:
            st.info("Digite um ID para carregar os dados para atualização.")

    elif selected_option == "Deletar Registro":
        st.header("Deletar Registro")
        record_id_to_delete = st.text_input("ID do Registro a Deletar")
        if st.button("Deletar"):
            if record_id_to_delete:
                try:
                    if st.session_state.db.delete_record(record_id_to_delete): # Using original DB
                        st.success(f"Registro {record_id_to_delete} deletado com sucesso!")
                    else:
                        st.error("Falha ao deletar o registro.")
                except Exception as e:
                    st.error(f"Erro ao deletar registro: {e}")
                    logging.error(f"Error deleting record: {e}", exc_info=True)
            else:
                st.error("Por favor, digite um ID para deletar.")

    elif selected_option == "Configurações":
        st.header("Configurações do Banco de Dados")
        st.subheader("Configurações do DB Original (Flat File)")
        st.write(f"Arquivo DB: {st.session_state.db.db_file}")
        st.write(f"Arquivo de Índice: {st.session_state.db.idx_file}")
        st.write(f"Próximo ID: {st.session_state.db._next_id}")

        if st.button("Fazer Backup Avançado (DB Original)"):
            st.session_state.db.create_backup_advanced(None) # Public key not implemented in demo
            st.info("Backup avançado concluído (sem criptografia real neste demo).")

        if st.button("Compactar Banco de Dados (DB Original)"):
            try:
                st.session_state.db.compact_database()
                st.success("Banco de dados original compactado com sucesso.")
            except Exception as e:
                st.error(f"Erro ao compactar banco de dados original: {e}")
                logging.error(f"Error compacting original DB: {e}", exc_info=True)

        # Opção para limpar o DB original
        if st.button("Limpar Arquivo de Banco de Dados Original"):
            if os.path.exists(st.session_state.db.db_file):
                try:
                    st.session_state.db._close_db() # Ensure file is closed
                    os.remove(st.session_state.db.db_file)
                    if os.path.exists(st.session_state.db.idx_file):
                        os.remove(st.session_state.db.idx_file)
                    if os.path.exists(st.session_state.db.id_counter_file):
                        os.remove(st.session_state.db.id_counter_file)
                    st.session_state.db = TrafficAccidentsDB(APP_CONFIG["DB_FILE_NAME"]) # Re-initialize
                    st.success("Banco de dados original limpo e reiniciado.")
                    st.experimental_rerun()
                except Exception as e:
                    st.error(f"Erro ao limpar banco de dados original: {e}")
                    logging.error(f"Error cleaning original DB: {e}", exc_info=True)

        st.subheader("Configurações do DB B-Tree (apenas implementado, não em uso)")
        st.write(f"Arquivo DB da B-Tree: {st.session_state.btree_db.pager.db_file_path}")
        st.write(f"Tamanho da Página: {PAGE_SIZE} bytes")
        st.write(f"Ordem da B-Tree: {st.session_state.btree_db.order}")
        st.write(f"Grau Mínimo (t): {st.session_state.btree_db.t}")

        # Opção para limpar o DB da B-Tree (apenas para testes/limpeza)
        if st.button("Limpar Arquivo de Banco de Dados B-Tree"):
            if os.path.exists(st.session_state.btree_db.pager.db_file_path):
                try:
                    st.session_state.btree_db.pager.flush_all()
                    st.session_state.btree_db.pager.file.close()
                    os.remove(st.session_state.btree_db.pager.db_file_path)
                    st.session_state.btree_db = TrafficAccidentsTree(APP_CONFIG["BTREE_DB_FILE_NAME"]) # Re-initialize
                    st.success("Banco de dados B-Tree limpo e reiniciado.")
                    st.experimental_rerun()
                except Exception as e:
                    st.error(f"Erro ao limpar banco de dados B-Tree: {e}")
                    logging.error(f"Error cleaning B-Tree DB: {e}", exc_info=True)


# Run the Streamlit UI
if __name__ == "__main__":
    setup_ui()

"""<div class="md-recitation">
  Sources
  <ol>
  <li><a href="https://github.com/4PROJ-5PROJ-Namkin/data-migration">https://github.com/4PROJ-5PROJ-Namkin/data-migration</a></li>
  <li><a href="https://github.com/bndl/bndl">https://github.com/bndl/bndl</a></li>
  </ol>
</div>
"""